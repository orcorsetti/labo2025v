{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/orcorsetti/labo2025v/blob/main/src/ensembles/420_ArbolesAzarosos_Bayesian.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Ensembles de Arboles de Decision"
      ],
      "metadata": {
        "id": "kgJ0E--L0n9s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un arbol de decisión es un modelo débil, el aumento del poder predictivo proviene al ensamblar varios arboles de decisión.\n",
        "<br> Si promedio n arboles identicos, el resultados es exactamente el mismo que utilizar un solo arbol, necesito PERTURBAR cada arbol para disponer de variablidad"
      ],
      "metadata": {
        "id": "PgLdmWznXWGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "la variabilidad provendrá de estas fuentes:\n",
        "\n",
        "\n",
        "*   Perturbar el dataset\n",
        "*   Perturbar el algoritmo del arbol\n",
        "*   Perturbar el dataset y el algoritmo del arbol al mismo tiempo"
      ],
      "metadata": {
        "id": "FTnxMEOqYRXZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se verán estos tres algoritmos\n",
        "\n",
        "\n",
        "*   Arboles Azarosos\n",
        "*   Random Forest\n",
        "*   Gradient Boosting of Decision Trees"
      ],
      "metadata": {
        "id": "DHp1h9m-X7Rc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m0ySYPfa7Zr"
      },
      "source": [
        "#### 4.01 Seteo del ambiente en Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGY7H9xza7Zr"
      },
      "source": [
        "Esta parte se debe correr con el runtime en Python3\n",
        "<br>Ir al menu, Runtime -> Change Runtime Type -> Runtime type ->  **Python 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PupIBNba7Zr"
      },
      "source": [
        "Conectar la virtual machine donde esta corriendo Google Colab con el  Google Drive, para poder tener persistencia de archivos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9LpZCst5a7Zs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8301088-b24d-4e00-ac41-6c73fb4ce954"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/.drive; to attempt to forcibly remount, call drive.mount(\"/content/.drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# primero establecer el Runtime de Python 3\n",
        "from google.colab import drive\n",
        "drive.mount('/content/.drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYC_F-wla7Zs"
      },
      "source": [
        "Para correr la siguiente celda es fundamental en Arranque en Frio haber copiado el archivo kaggle.json al Google Drive, en la carpeta indicada en el instructivo\n",
        "\n",
        "<br>los siguientes comando estan en shell script de Linux\n",
        "*   Crear las carpetas en el Google Drive\n",
        "*   \"instalar\" el archivo kaggle.json desde el Google Drive a la virtual machine para que pueda ser utilizado por la libreria  kaggle de Python\n",
        "*   Bajar el  **dataset_pequeno**  al  Google Drive  y tambien al disco local de la virtual machine que esta corriendo Google Colab\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XWLelftXa7Zt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cf86ca0-5237-4929-f748-65a8b60508c5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "mkdir -p \"/content/.drive/My Drive/labo1\"\n",
        "mkdir -p \"/content/buckets\"\n",
        "ln -s \"/content/.drive/My Drive/labo1\" /content/buckets/b1\n",
        "\n",
        "mkdir -p ~/.kaggle\n",
        "cp /content/buckets/b1/kaggle/kaggle.json  ~/.kaggle\n",
        "chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "\n",
        "mkdir -p /content/buckets/b1/exp\n",
        "mkdir -p /content/buckets/b1/datasets\n",
        "mkdir -p /content/datasets\n",
        "\n",
        "\n",
        "\n",
        "archivo_origen=\"https://storage.googleapis.com/open-courses/austral2025-af91/dataset_pequeno.csv\"\n",
        "archivo_destino=\"/content/datasets/dataset_pequeno.csv\"\n",
        "archivo_destino_bucket=\"/content/buckets/b1/datasets/dataset_pequeno.csv\"\n",
        "\n",
        "if ! test -f $archivo_destino_bucket; then\n",
        "  wget  $archivo_origen  -O $archivo_destino_bucket\n",
        "fi\n",
        "\n",
        "\n",
        "if ! test -f $archivo_destino; then\n",
        "  cp  $archivo_destino_bucket  $archivo_destino\n",
        "fi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pc9x9DnsNlZv"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.02 Arboles Azarosos"
      ],
      "metadata": {
        "id": "qq0KVOtq1K5D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Arboles Azarosos es el nombre de un algoritmo trivial (por favor NO confundir con Random Forest)\n",
        "Qué tipo de perturbaciones se realizan en Arboles Azarosos\n",
        "* Se perturba el dataset\n",
        "* No se perturba el algoritmo, es siempre rpart original"
      ],
      "metadata": {
        "id": "HsNJjhlRo9jM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cada  arbolito de  Arboles Azarosos se entrena sobre un dataset perturbado,  que tiene exactamente la misma cantidad de registros pero solo un subconjunto de los atributos (campos)  del dataset, tomados al azar, de los originales.\n",
        "<br> En esta primera corrida, se construira cada arbol en un dataset utilizando el 50% de los campos"
      ],
      "metadata": {
        "id": "rq2HC28CpXBR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSU5vi00CPRS"
      },
      "source": [
        "Esta parte se debe correr con el runtime en lenguaje **R** Ir al menu, Runtime -> Change Runtime Type -> Runtime type -> R"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zq8dySimCPRT"
      },
      "source": [
        "limpio el ambiente de R"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "format(Sys.time(), \"%a %b %d %X %Y\")"
      ],
      "metadata": {
        "id": "LrdtraBYJrsx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "20db4f87-bd18-4ba4-cadd-60cdd145f553"
      },
      "execution_count": 357,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "'Wed Oct 29 12:43:37 AM 2025'"
            ],
            "text/markdown": "'Wed Oct 29 12:43:37 AM 2025'",
            "text/latex": "'Wed Oct 29 12:43:37 AM 2025'",
            "text/plain": [
              "[1] \"Wed Oct 29 12:43:37 AM 2025\""
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 358,
      "metadata": {
        "id": "1iE0U4_WCPRT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "85a1e021-3a4b-4e1b-beba-dcb01356b2e1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A matrix: 2 × 6 of type dbl</caption>\n",
              "<thead>\n",
              "\t<tr><th></th><th scope=col>used</th><th scope=col>(Mb)</th><th scope=col>gc trigger</th><th scope=col>(Mb)</th><th scope=col>max used</th><th scope=col>(Mb)</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>Ncells</th><td>2672748</td><td>142.8</td><td>  4773622</td><td>255.0</td><td>  4773622</td><td> 255.0</td></tr>\n",
              "\t<tr><th scope=row>Vcells</th><td>4856309</td><td> 37.1</td><td>104883703</td><td>800.2</td><td>195324188</td><td>1490.3</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA matrix: 2 × 6 of type dbl\n\n| <!--/--> | used | (Mb) | gc trigger | (Mb) | max used | (Mb) |\n|---|---|---|---|---|---|---|\n| Ncells | 2672748 | 142.8 |   4773622 | 255.0 |   4773622 |  255.0 |\n| Vcells | 4856309 |  37.1 | 104883703 | 800.2 | 195324188 | 1490.3 |\n\n",
            "text/latex": "A matrix: 2 × 6 of type dbl\n\\begin{tabular}{r|llllll}\n  & used & (Mb) & gc trigger & (Mb) & max used & (Mb)\\\\\n\\hline\n\tNcells & 2672748 & 142.8 &   4773622 & 255.0 &   4773622 &  255.0\\\\\n\tVcells & 4856309 &  37.1 & 104883703 & 800.2 & 195324188 & 1490.3\\\\\n\\end{tabular}\n",
            "text/plain": [
              "       used    (Mb)  gc trigger (Mb)  max used  (Mb)  \n",
              "Ncells 2672748 142.8   4773622  255.0   4773622  255.0\n",
              "Vcells 4856309  37.1 104883703  800.2 195324188 1490.3"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# limpio la memoria\n",
        "rm(list=ls(all.names=TRUE)) # remove all objects\n",
        "gc(full=TRUE, verbose=FALSE) # garbage collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 359,
      "metadata": {
        "id": "BJDwdD0dCPRU"
      },
      "outputs": [],
      "source": [
        "# cargo las librerias que necesito\n",
        "require(\"data.table\")\n",
        "require(\"rpart\")\n",
        "require(\"parallel\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if( ! require(\"rlist\") ) install.packages(\"rlist\")\n",
        "require(\"rlist\")"
      ],
      "metadata": {
        "id": "Npcefp7r0wG6"
      },
      "execution_count": 360,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# paquete necesarios para la Bayesian Optimization\n",
        "if( !require(\"DiceKriging\") ) install.packages(\"DiceKriging\")\n",
        "require(\"DiceKriging\")"
      ],
      "metadata": {
        "id": "ViNckm6i0xk4"
      },
      "execution_count": 361,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# paquete necesarios para la Bayesian Optimization\n",
        "if( !require(\"mlrMBO\") ) install.packages(\"mlrMBO\")\n",
        "require(\"mlrMBO\")"
      ],
      "metadata": {
        "id": "GsUkXtFL00Q0"
      },
      "execution_count": 362,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui debe cargar SU semilla primigenia"
      ],
      "metadata": {
        "id": "M8-Pyp6CCPRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PARAM <- list()\n",
        "PARAM$semilla_primigenia <- 100183\n",
        "\n",
        "# parametros  arbol\n",
        "# entreno cada arbol con solo 50% de las variables variables\n",
        "#  por ahora, es fijo\n",
        "PARAM$feature_fraction <- 0.5\n",
        "\n",
        "PARAM$rpart$cp <- -1\n",
        "PARAM$rpart$minsplit <- 50\n",
        "PARAM$rpart$minbucket <- 20\n",
        "PARAM$rpart$maxdepth <- 6\n",
        "\n",
        "# voy a generar 512 arboles,\n",
        "#  a mas arboles mas tiempo de proceso y MEJOR MODELO,\n",
        "#  pero ganancias marginales\n",
        "PARAM$num_trees_max <- 2"
      ],
      "metadata": {
        "id": "peRH7ySLCPRV"
      },
      "execution_count": 404,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# carpeta de trabajo\n",
        "setwd(\"/content/buckets/b1/exp\")\n",
        "experimento <- \"exp4022\"\n",
        "dir.create(experimento, showWarnings=FALSE)\n",
        "setwd( paste0(\"/content/buckets/b1/exp/\", experimento ))"
      ],
      "metadata": {
        "id": "1gZD6ZMvCPRV"
      },
      "execution_count": 385,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lectura del dataset\n",
        "dataset <- fread(\"/content/datasets/dataset_pequeno.csv\")"
      ],
      "metadata": {
        "id": "Xi0emX2ECPRV"
      },
      "execution_count": 386,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defino los dataset de entrenamiento y aplicacion\n",
        "dtrain <- dataset[foto_mes == 202107]\n",
        "dfuture <- dataset[foto_mes == 202109]\n"
      ],
      "metadata": {
        "id": "RA3cSJ6KaGwA"
      },
      "execution_count": 387,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Establezco cuales son los campos que puedo usar para la prediccion\n",
        "# el copy() es por la Lazy Evaluation\n",
        "campos_buenos <- copy(setdiff(colnames(dtrain), c(\"clase_ternaria\")))"
      ],
      "metadata": {
        "id": "EByLkVMwaC8K"
      },
      "execution_count": 388,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set.seed(PARAM$semilla_primigenia) # Establezco la semilla aleatoria"
      ],
      "metadata": {
        "id": "H_DGUB_fhzHr"
      },
      "execution_count": 389,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "particionar <- function(data, division, agrupa = \"\", campo = \"fold\",\n",
        "                        start = 1, seed = NA) {\n",
        "  if (!is.na(seed)) set.seed(seed)\n",
        "\n",
        "  bloque <- unlist(mapply(\n",
        "    function(x, y) {\n",
        "      rep(y, x)\n",
        "    }, division,\n",
        "    seq(from= start, length.out= length(division))\n",
        "  ))\n",
        "\n",
        "  data[, (campo) := sample(rep(bloque, ceiling(.N / length(bloque))))[1:.N],\n",
        "    by= agrupa\n",
        "  ]\n",
        "}"
      ],
      "metadata": {
        "id": "T9ijr9JxTVX2"
      },
      "execution_count": 390,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EstimarGanancia = function(fold_test, x) {\n",
        "\n",
        "    tb_prediccion <- dfuture[, list(numero_de_cliente)]\n",
        "    # aqui se va acumulando la probabilidad del ensemble\n",
        "    tb_prediccion[, prob_acumulada := 0]\n",
        "\n",
        "    for (arbolito in seq(PARAM$num_trees_max) ) {\n",
        "    message( arbolito, \" \")\n",
        "    qty_campos_a_utilizar <- as.integer(length(campos_buenos)\n",
        "      * PARAM$feature_fraction)\n",
        "\n",
        "    # elijo los campos al azar\n",
        "    campos_random <- sample(campos_buenos, qty_campos_a_utilizar)\n",
        "\n",
        "    # paso de un vector a un string con los elementos\n",
        "    # separados por un signo de \"+\"\n",
        "    # este hace falta para la formula\n",
        "    campos_random <- paste(campos_random, collapse= \" + \")\n",
        "\n",
        "    # armo la formula para rpart\n",
        "    formulita <- paste0(\"clase_ternaria ~ \", campos_random)\n",
        "\n",
        "\n",
        "    # genero el arbol de decision\n",
        "    modelo <- rpart(formulita,\n",
        "      data= dataset[fold != fold_test, ],\n",
        "      xval= 0,\n",
        "      control= x\n",
        "    )\n",
        "\n",
        "    # aplico el modelo a los datos que no tienen clase\n",
        "    prediccion <- predict(modelo, dataset[fold == fold_test, ], type= \"prob\")\n",
        "\n",
        "    tb_prediccion[, prob_acumulada := prob_acumulada + prediccion[, \"BAJA+2\"]]\n",
        "\n",
        "    umbral_corte <- (1 / 40) * arbolito\n",
        "    #stopifnot(nrow(dfuture) == length(prob_baja2))\n",
        "    prob_baja2 <- tb_prediccion[, Predicted := as.numeric(prob_acumulada > umbral_corte)]\n",
        "\n",
        "    }\n",
        "\n",
        "\n",
        "    stopifnot(nrow(dataset[fold == fold_test, ]) == nrow(tb_prediccion))\n",
        "\n",
        "    # Agrego calculo la ganancia\n",
        "    ganancia_testing <- dataset[fold == fold_test][\n",
        "      prob_baja2 > 1 / 40,\n",
        "      sum(ifelse(clase_ternaria == \"BAJA+2\",\n",
        "        117000, -3000\n",
        "      ))\n",
        "    ]\n",
        "\n",
        "    message(ganancia_testing, \" \")\n",
        "\n",
        "    return(ganancia_testing)\n",
        "\n",
        "\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "j-xp2HQDhFmY"
      },
      "execution_count": 411,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ArbolesCrossValidation <- function(x) {\n",
        "  qfolds = 5\n",
        "  semilla= PARAM$semilla_primigenia\n",
        "  pagrupa= \"clase_ternaria\"\n",
        "\n",
        "  #GLOBAL_iteracion <<- GLOBAL_iteracion + 1\n",
        "\n",
        "  # generalmente  c(1, 1, 1, 1, 1 )  cinco unos\n",
        "  divi <- rep(1, qfolds)\n",
        "\n",
        "  # particiono en dataset en folds\n",
        "  particionar(dataset, divi, seed= semilla, agrupa= pagrupa)\n",
        "\n",
        "  ganancias <- mcmapply(EstimarGanancia,\n",
        "    seq(qfolds), # 1 2 3 4 5\n",
        "    MoreArgs= list(PARAM$rpart),\n",
        "    SIMPLIFY= FALSE,\n",
        "    mc.cores= detectCores()\n",
        "  )\n",
        "\n",
        "  dataset[, fold := NULL]\n",
        "\n",
        "  # devuelvo la primer ganancia y el promedio\n",
        "  # promedio las ganancias\n",
        "  ganancia_promedio <- mean(unlist(ganancias))\n",
        "  # aqui normalizo la ganancia\n",
        "  ganancia_promedio_normalizada <- ganancia_promedio * qfolds\n",
        "\n",
        "  message(ganancia_promedio_normalizada, \" \")\n",
        "#}"
      ],
      "metadata": {
        "id": "Hc8vQfDdTzLW",
        "outputId": "65f452a4-32e3-47ac-9c5b-c069b522d168",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 412,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning message in mclapply(seq_len(n), do_one, mc.preschedule = mc.preschedule, :\n",
            "“all scheduled cores encountered errors in user code”\n",
            "Warning message in mean.default(unlist(ganancias)):\n",
            "“argument is not numeric or logical: returning NA”\n",
            "NA \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "archivo_log <- \"BO_log.txt\"\n",
        "archivo_BO <- \"bayesian.RDATA\"\n",
        "\n",
        "# leo si ya existe el log\n",
        "#  para retomar en caso que se se corte el programa\n",
        "GLOBAL_iteracion <- 0\n",
        "GLOBAL_mejor <- -Inf\n",
        "\n",
        "if (file.exists(archivo_log)) {\n",
        "  tabla_log <- fread(archivo_log)\n",
        "  GLOBAL_iteracion <- nrow(tabla_log)\n",
        "  GLOBAL_mejor <- tabla_log[, max(ganancia)]\n",
        "}"
      ],
      "metadata": {
        "id": "GsllfIvUy3QS"
      },
      "execution_count": 395,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defino la  Optimizacion Bayesiana\n",
        "\n",
        "# cantidad de iteraciones de la Optimizacion Bayesiana\n",
        "PARAM <- list()\n",
        "\n",
        "PARAM$semilla_primigenia <- 100183\n",
        "PARAM$experimento <- \"HT312\"\n",
        "\n",
        "PARAM$BO_iter <- 2 #cantidad de iteraciones de la Bayesian Optimization\n",
        "\n",
        "# la letra L al final de 1L significa ENTERO\n",
        "PARAM$hs <- makeParamSet(\n",
        "    makeNumericParam(\"cp\", lower= -1, upper=-1),\n",
        "    makeIntegerParam(\"minsplit\", lower= 1, upper= 50),\n",
        "    makeIntegerParam(\"minbucket\", lower= 1, upper= 20),\n",
        "    makeIntegerParam(\"maxdepth\", lower= 3, upper= 5),\n",
        "    forbidden= quote(minbucket > 0.5 * minsplit)\n",
        ")\n",
        "# minbuket NO PUEDE ser mayor que la mitad de minsplit"
      ],
      "metadata": {
        "id": "3YTR0NYn03Vo"
      },
      "execution_count": 396,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# carpeta de trabajo\n",
        "setwd(\"/content/buckets/b1/exp\")\n",
        "dir.create(PARAM$experimento, showWarnings=FALSE)\n",
        "setwd( paste0(\"/content/buckets/b1/exp/\", PARAM$experimento ))"
      ],
      "metadata": {
        "id": "z3CQzlgV05ic"
      },
      "execution_count": 397,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aqui comienza la configuracion de la Bayesian Optimization\n",
        "\n",
        "funcion_optimizar <- ArbolesCrossValidation\n",
        "\n",
        "configureMlr(show.learner.output= FALSE)\n",
        "\n",
        "# configuro la busqueda bayesiana,\n",
        "#  los hiperparametros que se van a optimizar\n",
        "# por favor, no desesperarse por lo complejo\n",
        "# minimize= FALSE estoy Maximizando la ganancia\n",
        "obj.fun <- makeSingleObjectiveFunction(\n",
        "  fn= funcion_optimizar,\n",
        "  minimize= FALSE,\n",
        "  noisy= TRUE,\n",
        "  par.set= PARAM$hs,\n",
        "  has.simple.signature= FALSE\n",
        ")\n",
        "\n",
        "ctrl <- makeMBOControl(\n",
        "  save.on.disk.at.time= 600,\n",
        "  save.file.path= archivo_BO\n",
        ")\n",
        "\n",
        "ctrl <- setMBOControlTermination(ctrl, iters= PARAM$BO_iter)\n",
        "ctrl <- setMBOControlInfill(ctrl, crit= makeMBOInfillCritEI())\n",
        "\n",
        "surr.km <- makeLearner(\"regr.km\",\n",
        "  predict.type= \"se\",\n",
        "  covtype= \"matern3_2\", control= list(trace= TRUE)\n",
        ")"
      ],
      "metadata": {
        "id": "hDVl1lRYy7yd"
      },
      "execution_count": 398,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ctrl <- makeMBOControl()\n",
        "ctrl <- setMBOControlTermination(ctrl, iters = 100)\n",
        "ctrl <- setMBOControlInfill(ctrl, crit = makeMBOInfillCritEI())\n",
        "\n",
        "# Guarda automáticamente cada iteración\n",
        "ctrl$save.on.disk.at.time <- 60   # guarda cada 60 segundos\n",
        "ctrl$save.file.path <- archivo_BO # archivo .RData donde se guarda el estado\n",
        "\n",
        "# 4) Lanzar / reanudar\n",
        "if (!file.exists(archivo_BO)) {\n",
        "    bayesiana_salida <- mbo(\n",
        "    fun = obj.fun,\n",
        "    learner = surr.km,\n",
        "    control = ctrl\n",
        "  )\n",
        "} else {\n",
        "  bayesiana_salida <- mboContinue(archivo_BO)\n",
        "}"
      ],
      "metadata": {
        "id": "cK-V98uMCewm",
        "outputId": "602719ab-23db-44d5-8656-45f66950d4bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 399,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing y column(s) for design. Not provided.\n",
            "\n",
            "Warning message in mclapply(seq_len(n), do_one, mc.preschedule = mc.preschedule, :\n",
            "“all scheduled cores encountered errors in user code”\n",
            "Warning message in mean.default(unlist(ganancias)):\n",
            "“argument is not numeric or logical: returning NA”\n",
            "Warning message in mclapply(seq_len(n), do_one, mc.preschedule = mc.preschedule, :\n",
            "“all scheduled cores encountered errors in user code”\n",
            "Warning message in mean.default(unlist(ganancias)):\n",
            "“argument is not numeric or logical: returning NA”\n",
            "Warning message in mclapply(seq_len(n), do_one, mc.preschedule = mc.preschedule, :\n",
            "“all scheduled cores encountered errors in user code”\n",
            "Warning message in mean.default(unlist(ganancias)):\n",
            "“argument is not numeric or logical: returning NA”\n",
            "Warning message in mclapply(seq_len(n), do_one, mc.preschedule = mc.preschedule, :\n",
            "“all scheduled cores encountered errors in user code”\n",
            "Warning message in mean.default(unlist(ganancias)):\n",
            "“argument is not numeric or logical: returning NA”\n",
            "Warning message in mclapply(seq_len(n), do_one, mc.preschedule = mc.preschedule, :\n",
            "“all scheduled cores encountered errors in user code”\n",
            "Warning message in mean.default(unlist(ganancias)):\n",
            "“argument is not numeric or logical: returning NA”\n",
            "Warning message in mclapply(seq_len(n), do_one, mc.preschedule = mc.preschedule, :\n",
            "“all scheduled cores encountered errors in user code”\n",
            "Warning message in mean.default(unlist(ganancias)):\n",
            "“argument is not numeric or logical: returning NA”\n",
            "Warning message in mclapply(seq_len(n), do_one, mc.preschedule = mc.preschedule, :\n",
            "“all scheduled cores encountered errors in user code”\n",
            "Warning message in mean.default(unlist(ganancias)):\n",
            "“argument is not numeric or logical: returning NA”\n",
            "Warning message in mclapply(seq_len(n), do_one, mc.preschedule = mc.preschedule, :\n",
            "“all scheduled cores encountered errors in user code”\n",
            "Warning message in mean.default(unlist(ganancias)):\n",
            "“argument is not numeric or logical: returning NA”\n",
            "Warning message in mclapply(seq_len(n), do_one, mc.preschedule = mc.preschedule, :\n",
            "“all scheduled cores encountered errors in user code”\n",
            "Warning message in mean.default(unlist(ganancias)):\n",
            "“argument is not numeric or logical: returning NA”\n",
            "Warning message in mclapply(seq_len(n), do_one, mc.preschedule = mc.preschedule, :\n",
            "“all scheduled cores encountered errors in user code”\n",
            "Warning message in mean.default(unlist(ganancias)):\n",
            "“argument is not numeric or logical: returning NA”\n",
            "Warning message in mclapply(seq_len(n), do_one, mc.preschedule = mc.preschedule, :\n",
            "“all scheduled cores encountered errors in user code”\n",
            "Warning message in mean.default(unlist(ganancias)):\n",
            "“argument is not numeric or logical: returning NA”\n",
            "Warning message in mclapply(seq_len(n), do_one, mc.preschedule = mc.preschedule, :\n",
            "“all scheduled cores encountered errors in user code”\n",
            "Warning message in mean.default(unlist(ganancias)):\n",
            "“argument is not numeric or logical: returning NA”\n",
            "Warning message in mclapply(seq_len(n), do_one, mc.preschedule = mc.preschedule, :\n",
            "“all scheduled cores encountered errors in user code”\n",
            "Warning message in mean.default(unlist(ganancias)):\n",
            "“argument is not numeric or logical: returning NA”\n",
            "Warning message in mclapply(seq_len(n), do_one, mc.preschedule = mc.preschedule, :\n",
            "“all scheduled cores encountered errors in user code”\n",
            "Warning message in mean.default(unlist(ganancias)):\n",
            "“argument is not numeric or logical: returning NA”\n",
            "Warning message in mclapply(seq_len(n), do_one, mc.preschedule = mc.preschedule, :\n",
            "“all scheduled cores encountered errors in user code”\n",
            "Warning message in mean.default(unlist(ganancias)):\n",
            "“argument is not numeric or logical: returning NA”\n",
            "Warning message in mclapply(seq_len(n), do_one, mc.preschedule = mc.preschedule, :\n",
            "“all scheduled cores encountered errors in user code”\n",
            "Warning message in mean.default(unlist(ganancias)):\n",
            "“argument is not numeric or logical: returning NA”\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ERROR",
          "evalue": "Error in evalTargetFun.OptState(opt.state, xs, extras): Objective function output must be a numeric of length 1, but we got: NA\n",
          "traceback": [
            "Error in evalTargetFun.OptState(opt.state, xs, extras): Objective function output must be a numeric of length 1, but we got: NA\nTraceback:\n",
            "1. mboTemplate(opt.problem)",
            "2. mboTemplate.OptProblem(opt.problem)",
            "3. evalMBODesign.OptState(opt.state)",
            "4. evalTargetFun.OptState(opt.state, xs, extras)",
            "5. stopf(\"Objective function output must be a numeric of length %i, but we got: %s\", \n .     ny, convertToShortString(y))",
            "6. stop(obj)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inicio la optimizacion bayesiana\n",
        "if (!file.exists(archivo_BO)) {\n",
        "  bayesiana_salida <- mbo(\n",
        "    fun= obj.fun,\n",
        "    learner= surr.km,\n",
        "    control= ctrl\n",
        "  )\n",
        "} else {\n",
        "  bayesiana_salida <- mboContinue(archivo_BO)\n",
        "}\n",
        "# retomo en caso que ya exista"
      ],
      "metadata": {
        "id": "fcF4nzSZ0VsT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "118cf899-15a5-46f4-c6ed-a679a6ed5a81"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing y column(s) for design. Not provided.\n",
            "\n",
            "[mbo] 0: cp=-1; minsplit=7962; minbucket=1189; maxdepth=9 : y = 0 : 0.0 secs : initdesign\n",
            "\n",
            "[mbo] 0: cp=-1; minsplit=3666; minbucket=1360; maxdepth=15 : y = 0 : 0.0 secs : initdesign\n",
            "\n",
            "[mbo] 0: cp=-1; minsplit=4076; minbucket=1704; maxdepth=4 : y = 0 : 0.0 secs : initdesign\n",
            "\n",
            "[mbo] 0: cp=-1; minsplit=5951; minbucket=289; maxdepth=16 : y = 0 : 0.0 secs : initdesign\n",
            "\n",
            "[mbo] 0: cp=-1; minsplit=4777; minbucket=1950; maxdepth=6 : y = 0 : 0.0 secs : initdesign\n",
            "\n",
            "[mbo] 0: cp=-1; minsplit=7275; minbucket=162; maxdepth=6 : y = 0 : 0.0 secs : initdesign\n",
            "\n",
            "[mbo] 0: cp=-1; minsplit=6141; minbucket=2750; maxdepth=11 : y = 0 : 0.0 secs : initdesign\n",
            "\n",
            "[mbo] 0: cp=-1; minsplit=6426; minbucket=2059; maxdepth=19 : y = 0 : 0.0 secs : initdesign\n",
            "\n",
            "[mbo] 0: cp=-1; minsplit=7505; minbucket=2944; maxdepth=8 : y = 0 : 0.0 secs : initdesign\n",
            "\n",
            "[mbo] 0: cp=-1; minsplit=6001; minbucket=987; maxdepth=14 : y = 0 : 0.0 secs : initdesign\n",
            "\n",
            "[mbo] 0: cp=-1; minsplit=1143; minbucket=457; maxdepth=15 : y = 0 : 0.0 secs : initdesign\n",
            "\n",
            "[mbo] 0: cp=-1; minsplit=3673; minbucket=1578; maxdepth=5 : y = 0 : 0.0 secs : initdesign\n",
            "\n",
            "[mbo] 0: cp=-1; minsplit=5193; minbucket=33; maxdepth=4 : y = 0 : 0.0 secs : initdesign\n",
            "\n",
            "[mbo] 0: cp=-1; minsplit=6312; minbucket=1416; maxdepth=7 : y = 0 : 0.0 secs : initdesign\n",
            "\n",
            "[mbo] 0: cp=-1; minsplit=5972; minbucket=1130; maxdepth=18 : y = 0 : 0.0 secs : initdesign\n",
            "\n",
            "[mbo] 0: cp=-1; minsplit=5887; minbucket=554; maxdepth=18 : y = 0 : 0.0 secs : initdesign\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ERROR",
          "evalue": "Error in backsolve(t(T), c.newdata, upper.tri = FALSE): singular matrix in 'backsolve'. First zero in diagonal [1]\n",
          "traceback": [
            "Error in backsolve(t(T), c.newdata, upper.tri = FALSE): singular matrix in 'backsolve'. First zero in diagonal [1]\nTraceback:\n",
            "1. mboTemplate(opt.problem)",
            "2. mboTemplate.OptProblem(opt.problem)",
            "3. mboTemplate(opt.state)",
            "4. mboTemplate.OptState(opt.state)",
            "5. proposePoints(opt.state)",
            "6. proposePointsByInfillOptimization(opt.state)",
            "7. measureTime({\n .     prop.points = infill.opt.fun(infill.crit.fun, models = models, \n .         control = control, par.set = par.set, opt.path = opt.path, \n .         designs = designs, iter = iter, progress = progress, \n .         ...)\n . })",
            "8. force(expr)",
            "9. infill.opt.fun(infill.crit.fun, models = models, control = control, \n .     par.set = par.set, opt.path = opt.path, designs = designs, \n .     iter = iter, progress = progress, ...)",
            "10. infill.crit(newdesign, models, control, ps.local, designs, iter, \n  .     ...)",
            "11. predict(model, newdata = points)",
            "12. predict.WrappedModel(model, newdata = points)",
            "13. measureTime(fun1({\n  .     p = fun2(fun3(do.call(predictLearner2, pars)))\n  . }))",
            "14. force(expr)",
            "15. fun1({\n  .     p = fun2(fun3(do.call(predictLearner2, pars)))\n  . })",
            "16. withVisible(...elt(i))",
            "17. fun2(fun3(do.call(predictLearner2, pars)))",
            "18. fun3(do.call(predictLearner2, pars))",
            "19. do.call(predictLearner2, pars)",
            "20. (function (.learner, .model, .newdata, ...) \n  . {\n  .     if (.learner$fix.factors.prediction) {\n  .         fls = .model$factor.levels\n  .         ns = names(fls)\n  .         ns = intersect(colnames(.newdata), ns)\n  .         fls = fls[ns]\n  .         if (length(ns) > 0L) {\n  .             safe_factor = function(x, levels) {\n  .                 if (length(setdiff(levels(x), levels)) > 0) {\n  .                   warning(\"fix.factors.prediction = TRUE produced NAs because of new factor levels in prediction data.\")\n  .                 }\n  .                 factor(x, levels)\n  .             }\n  .             .newdata[ns] = mapply(safe_factor, x = .newdata[ns], \n  .                 levels = fls, SIMPLIFY = FALSE)\n  .         }\n  .     }\n  .     p = predictLearner(.learner, .model, .newdata, ...)\n  .     p = checkPredictLearnerOutput(.learner, .model, p)\n  .     return(p)\n  . })(.learner = structure(list(id = \"regr.km\", type = \"regr\", package = \"DiceKriging\", \n  .     properties = c(\"numerics\", \"se\"), par.set = structure(list(\n  .         pars = list(covtype = structure(list(id = \"covtype\", \n  .             type = \"discrete\", len = 1L, lower = NULL, upper = NULL, \n  .             values = list(gauss = \"gauss\", matern5_2 = \"matern5_2\", \n  .                 matern3_2 = \"matern3_2\", exp = \"exp\", powexp = \"powexp\"), \n  .             cnames = NULL, allow.inf = FALSE, has.default = TRUE, \n  .             default = \"matern5_2\", trafo = NULL, requires = NULL, \n  .             tunable = TRUE, special.vals = list(), when = \"train\"), class = c(\"LearnerParam\", \n  .         \"Param\")), coef.trend = structure(list(id = \"coef.trend\", \n  .             type = \"numericvector\", len = NA_integer_, lower = -Inf, \n  .             upper = Inf, values = NULL, cnames = NULL, allow.inf = FALSE, \n  .             has.default = FALSE, default = NULL, trafo = NULL, \n  .             requires = NULL, tunable = TRUE, special.vals = list(), \n  .             when = \"train\"), class = c(\"LearnerParam\", \"Param\"\n  .         )), coef.cov = structure(list(id = \"coef.cov\", type = \"numericvector\", \n  .             len = NA_integer_, lower = -Inf, upper = Inf, values = NULL, \n  .             cnames = NULL, allow.inf = FALSE, has.default = FALSE, \n  .             default = NULL, trafo = NULL, requires = NULL, tunable = TRUE, \n  .             special.vals = list(), when = \"train\"), class = c(\"LearnerParam\", \n  .         \"Param\")), coef.var = structure(list(id = \"coef.var\", \n  .             type = \"numericvector\", len = NA_integer_, lower = -Inf, \n  .             upper = Inf, values = NULL, cnames = NULL, allow.inf = FALSE, \n  .             has.default = FALSE, default = NULL, trafo = NULL, \n  .             requires = NULL, tunable = TRUE, special.vals = list(), \n  .             when = \"train\"), class = c(\"LearnerParam\", \"Param\"\n  .         )), nugget = structure(list(id = \"nugget\", type = \"numeric\", \n  .             len = 1L, lower = -Inf, upper = Inf, values = NULL, \n  .             cnames = NULL, allow.inf = FALSE, has.default = FALSE, \n  .             default = NULL, trafo = NULL, requires = NULL, tunable = TRUE, \n  .             special.vals = list(), when = \"train\"), class = c(\"LearnerParam\", \n  .         \"Param\")), nugget.estim = structure(list(id = \"nugget.estim\", \n  .             type = \"logical\", len = 1L, lower = NULL, upper = NULL, \n  .             values = list(`TRUE` = TRUE, `FALSE` = FALSE), cnames = NULL, \n  .             allow.inf = FALSE, has.default = TRUE, default = FALSE, \n  .             trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list(), \n  .             when = \"train\"), class = c(\"LearnerParam\", \"Param\"\n  .         )), noise.var = structure(list(id = \"noise.var\", type = \"numericvector\", \n  .             len = NA_integer_, lower = -Inf, upper = Inf, values = NULL, \n  .             cnames = NULL, allow.inf = FALSE, has.default = FALSE, \n  .             default = NULL, trafo = NULL, requires = NULL, tunable = TRUE, \n  .             special.vals = list(), when = \"train\"), class = c(\"LearnerParam\", \n  .         \"Param\")), estim.method = structure(list(id = \"estim.method\", \n  .             type = \"discrete\", len = 1L, lower = NULL, upper = NULL, \n  .             values = list(MLE = \"MLE\", LOO = \"LOO\"), cnames = NULL, \n  .             allow.inf = FALSE, has.default = TRUE, default = \"MLE\", \n  .             trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list(), \n  .             when = \"train\"), class = c(\"LearnerParam\", \"Param\"\n  .         )), optim.method = structure(list(id = \"optim.method\", \n  .             type = \"discrete\", len = 1L, lower = NULL, upper = NULL, \n  .             values = list(BFGS = \"BFGS\", gen = \"gen\"), cnames = NULL, \n  .             allow.inf = FALSE, has.default = TRUE, default = \"BFGS\", \n  .             trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list(), \n  .             when = \"train\"), class = c(\"LearnerParam\", \"Param\"\n  .         )), lower = structure(list(id = \"lower\", type = \"numericvector\", \n  .             len = NA_integer_, lower = -Inf, upper = Inf, values = NULL, \n  .             cnames = NULL, allow.inf = FALSE, has.default = FALSE, \n  .             default = NULL, trafo = NULL, requires = NULL, tunable = TRUE, \n  .             special.vals = list(), when = \"train\"), class = c(\"LearnerParam\", \n  .         \"Param\")), upper = structure(list(id = \"upper\", type = \"numericvector\", \n  .             len = NA_integer_, lower = -Inf, upper = Inf, values = NULL, \n  .             cnames = NULL, allow.inf = FALSE, has.default = FALSE, \n  .             default = NULL, trafo = NULL, requires = NULL, tunable = TRUE, \n  .             special.vals = list(), when = \"train\"), class = c(\"LearnerParam\", \n  .         \"Param\")), parinit = structure(list(id = \"parinit\", type = \"numericvector\", \n  .             len = NA_integer_, lower = -Inf, upper = Inf, values = NULL, \n  .             cnames = NULL, allow.inf = FALSE, has.default = FALSE, \n  .             default = NULL, trafo = NULL, requires = NULL, tunable = TRUE, \n  .             special.vals = list(), when = \"train\"), class = c(\"LearnerParam\", \n  .         \"Param\")), multistart = structure(list(id = \"multistart\", \n  .             type = \"integer\", len = 1L, lower = 1L, upper = Inf, \n  .             values = NULL, cnames = NULL, allow.inf = FALSE, \n  .             has.default = TRUE, default = 1L, trafo = NULL, requires = NULL, \n  .             tunable = TRUE, special.vals = list(), when = \"train\"), class = c(\"LearnerParam\", \n  .         \"Param\")), control = structure(list(id = \"control\", type = \"untyped\", \n  .             len = 1L, lower = NULL, upper = NULL, values = NULL, \n  .             cnames = NULL, allow.inf = FALSE, has.default = FALSE, \n  .             default = NULL, trafo = NULL, requires = NULL, tunable = TRUE, \n  .             special.vals = list(), when = \"train\"), class = c(\"LearnerParam\", \n  .         \"Param\")), gr = structure(list(id = \"gr\", type = \"logical\", \n  .             len = 1L, lower = NULL, upper = NULL, values = list(\n  .                 `TRUE` = TRUE, `FALSE` = FALSE), cnames = NULL, \n  .             allow.inf = FALSE, has.default = TRUE, default = TRUE, \n  .             trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list(), \n  .             when = \"train\"), class = c(\"LearnerParam\", \"Param\"\n  .         )), iso = structure(list(id = \"iso\", type = \"logical\", \n  .             len = 1L, lower = NULL, upper = NULL, values = list(\n  .                 `TRUE` = TRUE, `FALSE` = FALSE), cnames = NULL, \n  .             allow.inf = FALSE, has.default = TRUE, default = FALSE, \n  .             trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list(), \n  .             when = \"train\"), class = c(\"LearnerParam\", \"Param\"\n  .         )), scaling = structure(list(id = \"scaling\", type = \"logical\", \n  .             len = 1L, lower = NULL, upper = NULL, values = list(\n  .                 `TRUE` = TRUE, `FALSE` = FALSE), cnames = NULL, \n  .             allow.inf = FALSE, has.default = TRUE, default = FALSE, \n  .             trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list(), \n  .             when = \"train\"), class = c(\"LearnerParam\", \"Param\"\n  .         )), knots = structure(list(id = \"knots\", type = \"untyped\", \n  .             len = 1L, lower = NULL, upper = NULL, values = NULL, \n  .             cnames = NULL, allow.inf = FALSE, has.default = FALSE, \n  .             default = NULL, trafo = NULL, requires = NULL, tunable = TRUE, \n  .             special.vals = list(), when = \"train\"), class = c(\"LearnerParam\", \n  .         \"Param\")), jitter = structure(list(id = \"jitter\", type = \"logical\", \n  .             len = 1L, lower = NULL, upper = NULL, values = list(\n  .                 `TRUE` = TRUE, `FALSE` = FALSE), cnames = NULL, \n  .             allow.inf = FALSE, has.default = TRUE, default = FALSE, \n  .             trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list(), \n  .             when = \"predict\"), class = c(\"LearnerParam\", \"Param\"\n  .         )), nugget.stability = structure(list(id = \"nugget.stability\", \n  .             type = \"numeric\", len = 1L, lower = -Inf, upper = Inf, \n  .             values = NULL, cnames = NULL, allow.inf = FALSE, \n  .             has.default = FALSE, default = NULL, trafo = NULL, \n  .             requires = !nugget.estim && is.null(nugget), tunable = TRUE, \n  .             special.vals = list(), when = \"train\"), class = c(\"LearnerParam\", \n  .         \"Param\"))), forbidden = NULL), class = c(\"LearnerParamSet\", \n  .     \"ParamSet\")), par.vals = list(jitter = FALSE, covtype = \"matern3_2\", \n  .         control = list(trace = TRUE)), predict.type = \"se\", cache = FALSE, \n  .     name = \"Kriging\", short.name = \"km\", note = \"In predict, we currently always use `type = \\\"SK\\\"`. The extra parameter `jitter` (default is `FALSE`) enables adding a very small jitter (order 1e-12) to the x-values before prediction, as `predict.km` reproduces the exact y-values of the training data points, when you pass them in, even if the nugget effect is turned on. \\n We further introduced `nugget.stability` which sets the `nugget` to `nugget.stability * var(y)` before each training to improve numerical stability. We recommend a setting of 10^-8\", \n  .     callees = \"km\", help.list = list(covtype = \"Argument of: DiceKriging::km\\n\\nan optional character string specifying the covariance structure to be used, to be chosen between \\\"gauss\\\", \\\"matern5_2\\\", \\\"matern3_2\\\", \\\"exp\\\" or \\\"powexp\\\". See a full description of available covariance kernels in covTensorProduct-class. Default is \\\"matern5_2\\\". See also the argument kernel that allows the user to build its own covariance structure.\", \n  .         coef.trend = \"Argument of: DiceKriging::km\\n\\n(see below)\", \n  .         coef.cov = \"Argument of: DiceKriging::km\\n\\n(see below)\", \n  .         coef.var = \"Argument of: DiceKriging::km\\n\\noptional vectors containing the values for the trend, covariance and variance parameters. For estimation, 4 cases are implemented: 1. (All unknown) If all are missing, all are estimated. 2. (All known) If all are provided, no estimation is performed; 3. (Known trend) If coef.trend is provided but at least one of coef.cov or coef.var is missing, then BOTH coef.cov and coef.var are estimated; 4. (Unknown trend) If coef.cov and coef.var are provided but coef.trend is missing, then coef.trend is estimated (GLS formula).\", \n  .         nugget = \"Argument of: DiceKriging::km\\n\\nan optional variance value standing for the homogeneous nugget effect.\", \n  .         nugget.estim = \"Argument of: DiceKriging::km\\n\\nan optional boolean indicating whether the nugget effect should be estimated. Note that this option does not concern the case of heterogeneous noisy observations (see noise.var below). If nugget is given, it is used as an initial value. Default is FALSE.\", \n  .         noise.var = \"Argument of: DiceKriging::km\\n\\nfor noisy observations : an optional vector containing the noise variance at each observation. This is useful for stochastic simulators. Default is NULL.\", \n  .         estim.method = \"Argument of: DiceKriging::km\\n\\na character string specifying the method by which unknown parameters are estimated. Default is \\\"MLE\\\" (Maximum Likelihood). At this stage, a beta version of leave-One-Out estimation (estim.method=\\\"LOO\\\") is also implemented for noise-free observations.\", \n  .         optim.method = \"Argument of: DiceKriging::km\\n\\nan optional character string indicating which optimization method is chosen for the likelihood maximization. \\\"BFGS\\\" is the optim quasi-Newton procedure of package stats, with the method \\\"L-BFGS-B\\\". \\\"gen\\\" is the genoud genetic algorithm (using derivatives) from package rgenoud (>= 5.3.3).\", \n  .         lower = \"Argument of: DiceKriging::km\\n\\n(see below)\", \n  .         upper = \"Argument of: DiceKriging::km\\n\\noptional vectors containing the bounds of the correlation parameters for optimization. The default values are given by covParametersBounds.\", \n  .         parinit = \"Argument of: DiceKriging::km\\n\\nan optional vector containing the initial values for the variables to be optimized over. If no vector is given, an initial point is generated as follows. For method \\\"gen\\\", the initial point is generated uniformly inside the hyper-rectangle domain defined by lower and upper. For method \\\"BFGS\\\", some points (see control below) are generated uniformly in the domain. Then the best point with respect to the likelihood (or penalized likelihood, see penalty) criterion is chosen.\", \n  .         multistart = \"Argument of: DiceKriging::km\\n\\nan optional integer indicating the number of initial points from which running the BFGS optimizer. These points will be selected as the best multistart one(s) among those evaluated (see above parinit). The multiple optimizations will be performed in parallel provided that a parallel backend is registered (see package foreach).\", \n  .         control = \"Argument of: DiceKriging::km\\n\\nan optional list of control parameters for optimization. See details below.\", \n  .         gr = \"Argument of: DiceKriging::km\\n\\nan optional boolean indicating whether the analytical gradient should be used. Default is TRUE.\", \n  .         iso = \"Argument of: DiceKriging::km\\n\\nan optional boolean that can be used to force a tensor-product covariance structure (see covTensorProduct-class) to have a range parameter common to all dimensions. Default is FALSE. Not used (at this stage) for the power-exponential type.\", \n  .         scaling = \"Argument of: DiceKriging::km\\n\\nan optional boolean indicating whether a scaling on the covariance structure should be used.\", \n  .         knots = \"Argument of: DiceKriging::km\\n\\nan optional list of knots for scaling. The j-th element is a vector containing the knots for dimension j. If scaling=TRUE and knots are not specified, than knots are fixed to 0 and 1 in each dimension (which corresponds to affine scaling for the domain [0,1]^d).\"), \n  .     config = list(on.learner.error = \"stop\"), fix.factors.prediction = TRUE), class = c(\"regr.km\", \n  . \"RLearnerRegr\", \"RLearner\", \"Learner\")), .model = structure(list(\n  .     learner = structure(list(id = \"regr.km\", type = \"regr\", package = \"DiceKriging\", \n  .         properties = c(\"numerics\", \"se\"), par.set = structure(list(\n  .             pars = list(covtype = structure(list(id = \"covtype\", \n  .                 type = \"discrete\", len = 1L, lower = NULL, upper = NULL, \n  .                 values = list(gauss = \"gauss\", matern5_2 = \"matern5_2\", \n  .                   matern3_2 = \"matern3_2\", exp = \"exp\", powexp = \"powexp\"), \n  .                 cnames = NULL, allow.inf = FALSE, has.default = TRUE, \n  .                 default = \"matern5_2\", trafo = NULL, requires = NULL, \n  .                 tunable = TRUE, special.vals = list(), when = \"train\"), class = c(\"LearnerParam\", \n  .             \"Param\")), coef.trend = structure(list(id = \"coef.trend\", \n  .                 type = \"numericvector\", len = NA_integer_, lower = -Inf, \n  .                 upper = Inf, values = NULL, cnames = NULL, allow.inf = FALSE, \n  .                 has.default = FALSE, default = NULL, trafo = NULL, \n  .                 requires = NULL, tunable = TRUE, special.vals = list(), \n  .                 when = \"train\"), class = c(\"LearnerParam\", \"Param\"\n  .             )), coef.cov = structure(list(id = \"coef.cov\", type = \"numericvector\", \n  .                 len = NA_integer_, lower = -Inf, upper = Inf, \n  .                 values = NULL, cnames = NULL, allow.inf = FALSE, \n  .                 has.default = FALSE, default = NULL, trafo = NULL, \n  .                 requires = NULL, tunable = TRUE, special.vals = list(), \n  .                 when = \"train\"), class = c(\"LearnerParam\", \"Param\"\n  .             )), coef.var = structure(list(id = \"coef.var\", type = \"numericvector\", \n  .                 len = NA_integer_, lower = -Inf, upper = Inf, \n  .                 values = NULL, cnames = NULL, allow.inf = FALSE, \n  .                 has.default = FALSE, default = NULL, trafo = NULL, \n  .                 requires = NULL, tunable = TRUE, special.vals = list(), \n  .                 when = \"train\"), class = c(\"LearnerParam\", \"Param\"\n  .             )), nugget = structure(list(id = \"nugget\", type = \"numeric\", \n  .                 len = 1L, lower = -Inf, upper = Inf, values = NULL, \n  .                 cnames = NULL, allow.inf = FALSE, has.default = FALSE, \n  .                 default = NULL, trafo = NULL, requires = NULL, \n  .                 tunable = TRUE, special.vals = list(), when = \"train\"), class = c(\"LearnerParam\", \n  .             \"Param\")), nugget.estim = structure(list(id = \"nugget.estim\", \n  .                 type = \"logical\", len = 1L, lower = NULL, upper = NULL, \n  .                 values = list(`TRUE` = TRUE, `FALSE` = FALSE), \n  .                 cnames = NULL, allow.inf = FALSE, has.default = TRUE, \n  .                 default = FALSE, trafo = NULL, requires = NULL, \n  .                 tunable = TRUE, special.vals = list(), when = \"train\"), class = c(\"LearnerParam\", \n  .             \"Param\")), noise.var = structure(list(id = \"noise.var\", \n  .                 type = \"numericvector\", len = NA_integer_, lower = -Inf, \n  .                 upper = Inf, values = NULL, cnames = NULL, allow.inf = FALSE, \n  .                 has.default = FALSE, default = NULL, trafo = NULL, \n  .                 requires = NULL, tunable = TRUE, special.vals = list(), \n  .                 when = \"train\"), class = c(\"LearnerParam\", \"Param\"\n  .             )), estim.method = structure(list(id = \"estim.method\", \n  .                 type = \"discrete\", len = 1L, lower = NULL, upper = NULL, \n  .                 values = list(MLE = \"MLE\", LOO = \"LOO\"), cnames = NULL, \n  .                 allow.inf = FALSE, has.default = TRUE, default = \"MLE\", \n  .                 trafo = NULL, requires = NULL, tunable = TRUE, \n  .                 special.vals = list(), when = \"train\"), class = c(\"LearnerParam\", \n  .             \"Param\")), optim.method = structure(list(id = \"optim.method\", \n  .                 type = \"discrete\", len = 1L, lower = NULL, upper = NULL, \n  .                 values = list(BFGS = \"BFGS\", gen = \"gen\"), cnames = NULL, \n  .                 allow.inf = FALSE, has.default = TRUE, default = \"BFGS\", \n  .                 trafo = NULL, requires = NULL, tunable = TRUE, \n  .                 special.vals = list(), when = \"train\"), class = c(\"LearnerParam\", \n  .             \"Param\")), lower = structure(list(id = \"lower\", type = \"numericvector\", \n  .                 len = NA_integer_, lower = -Inf, upper = Inf, \n  .                 values = NULL, cnames = NULL, allow.inf = FALSE, \n  .                 has.default = FALSE, default = NULL, trafo = NULL, \n  .                 requires = NULL, tunable = TRUE, special.vals = list(), \n  .                 when = \"train\"), class = c(\"LearnerParam\", \"Param\"\n  .             )), upper = structure(list(id = \"upper\", type = \"numericvector\", \n  .                 len = NA_integer_, lower = -Inf, upper = Inf, \n  .                 values = NULL, cnames = NULL, allow.inf = FALSE, \n  .                 has.default = FALSE, default = NULL, trafo = NULL, \n  .                 requires = NULL, tunable = TRUE, special.vals = list(), \n  .                 when = \"train\"), class = c(\"LearnerParam\", \"Param\"\n  .             )), parinit = structure(list(id = \"parinit\", type = \"numericvector\", \n  .                 len = NA_integer_, lower = -Inf, upper = Inf, \n  .                 values = NULL, cnames = NULL, allow.inf = FALSE, \n  .                 has.default = FALSE, default = NULL, trafo = NULL, \n  .                 requires = NULL, tunable = TRUE, special.vals = list(), \n  .                 when = \"train\"), class = c(\"LearnerParam\", \"Param\"\n  .             )), multistart = structure(list(id = \"multistart\", \n  .                 type = \"integer\", len = 1L, lower = 1L, upper = Inf, \n  .                 values = NULL, cnames = NULL, allow.inf = FALSE, \n  .                 has.default = TRUE, default = 1L, trafo = NULL, \n  .                 requires = NULL, tunable = TRUE, special.vals = list(), \n  .                 when = \"train\"), class = c(\"LearnerParam\", \"Param\"\n  .             )), control = structure(list(id = \"control\", type = \"untyped\", \n  .                 len = 1L, lower = NULL, upper = NULL, values = NULL, \n  .                 cnames = NULL, allow.inf = FALSE, has.default = FALSE, \n  .                 default = NULL, trafo = NULL, requires = NULL, \n  .                 tunable = TRUE, special.vals = list(), when = \"train\"), class = c(\"LearnerParam\", \n  .             \"Param\")), gr = structure(list(id = \"gr\", type = \"logical\", \n  .                 len = 1L, lower = NULL, upper = NULL, values = list(\n  .                   `TRUE` = TRUE, `FALSE` = FALSE), cnames = NULL, \n  .                 allow.inf = FALSE, has.default = TRUE, default = TRUE, \n  .                 trafo = NULL, requires = NULL, tunable = TRUE, \n  .                 special.vals = list(), when = \"train\"), class = c(\"LearnerParam\", \n  .             \"Param\")), iso = structure(list(id = \"iso\", type = \"logical\", \n  .                 len = 1L, lower = NULL, upper = NULL, values = list(\n  .                   `TRUE` = TRUE, `FALSE` = FALSE), cnames = NULL, \n  .                 allow.inf = FALSE, has.default = TRUE, default = FALSE, \n  .                 trafo = NULL, requires = NULL, tunable = TRUE, \n  .                 special.vals = list(), when = \"train\"), class = c(\"LearnerParam\", \n  .             \"Param\")), scaling = structure(list(id = \"scaling\", \n  .                 type = \"logical\", len = 1L, lower = NULL, upper = NULL, \n  .                 values = list(`TRUE` = TRUE, `FALSE` = FALSE), \n  .                 cnames = NULL, allow.inf = FALSE, has.default = TRUE, \n  .                 default = FALSE, trafo = NULL, requires = NULL, \n  .                 tunable = TRUE, special.vals = list(), when = \"train\"), class = c(\"LearnerParam\", \n  .             \"Param\")), knots = structure(list(id = \"knots\", type = \"untyped\", \n  .                 len = 1L, lower = NULL, upper = NULL, values = NULL, \n  .                 cnames = NULL, allow.inf = FALSE, has.default = FALSE, \n  .                 default = NULL, trafo = NULL, requires = NULL, \n  .                 tunable = TRUE, special.vals = list(), when = \"train\"), class = c(\"LearnerParam\", \n  .             \"Param\")), jitter = structure(list(id = \"jitter\", \n  .                 type = \"logical\", len = 1L, lower = NULL, upper = NULL, \n  .                 values = list(`TRUE` = TRUE, `FALSE` = FALSE), \n  .                 cnames = NULL, allow.inf = FALSE, has.default = TRUE, \n  .                 default = FALSE, trafo = NULL, requires = NULL, \n  .                 tunable = TRUE, special.vals = list(), when = \"predict\"), class = c(\"LearnerParam\", \n  .             \"Param\")), nugget.stability = structure(list(id = \"nugget.stability\", \n  .                 type = \"numeric\", len = 1L, lower = -Inf, upper = Inf, \n  .                 values = NULL, cnames = NULL, allow.inf = FALSE, \n  .                 has.default = FALSE, default = NULL, trafo = NULL, \n  .                 requires = !nugget.estim && is.null(nugget), \n  .                 tunable = TRUE, special.vals = list(), when = \"train\"), class = c(\"LearnerParam\", \n  .             \"Param\"))), forbidden = NULL), class = c(\"LearnerParamSet\", \n  .         \"ParamSet\")), par.vals = list(jitter = FALSE, covtype = \"matern3_2\", \n  .             control = list(trace = TRUE)), predict.type = \"se\", \n  .         cache = FALSE, name = \"Kriging\", short.name = \"km\", note = \"In predict, we currently always use `type = \\\"SK\\\"`. The extra parameter `jitter` (default is `FALSE`) enables adding a very small jitter (order 1e-12) to the x-values before prediction, as `predict.km` reproduces the exact y-values of the training data points, when you pass them in, even if the nugget effect is turned on. \\n We further introduced `nugget.stability` which sets the `nugget` to `nugget.stability * var(y)` before each training to improve numerical stability. We recommend a setting of 10^-8\", \n  .         callees = \"km\", help.list = list(covtype = \"Argument of: DiceKriging::km\\n\\nan optional character string specifying the covariance structure to be used, to be chosen between \\\"gauss\\\", \\\"matern5_2\\\", \\\"matern3_2\\\", \\\"exp\\\" or \\\"powexp\\\". See a full description of available covariance kernels in covTensorProduct-class. Default is \\\"matern5_2\\\". See also the argument kernel that allows the user to build its own covariance structure.\", \n  .             coef.trend = \"Argument of: DiceKriging::km\\n\\n(see below)\", \n  .             coef.cov = \"Argument of: DiceKriging::km\\n\\n(see below)\", \n  .             coef.var = \"Argument of: DiceKriging::km\\n\\noptional vectors containing the values for the trend, covariance and variance parameters. For estimation, 4 cases are implemented: 1. (All unknown) If all are missing, all are estimated. 2. (All known) If all are provided, no estimation is performed; 3. (Known trend) If coef.trend is provided but at least one of coef.cov or coef.var is missing, then BOTH coef.cov and coef.var are estimated; 4. (Unknown trend) If coef.cov and coef.var are provided but coef.trend is missing, then coef.trend is estimated (GLS formula).\", \n  .             nugget = \"Argument of: DiceKriging::km\\n\\nan optional variance value standing for the homogeneous nugget effect.\", \n  .             nugget.estim = \"Argument of: DiceKriging::km\\n\\nan optional boolean indicating whether the nugget effect should be estimated. Note that this option does not concern the case of heterogeneous noisy observations (see noise.var below). If nugget is given, it is used as an initial value. Default is FALSE.\", \n  .             noise.var = \"Argument of: DiceKriging::km\\n\\nfor noisy observations : an optional vector containing the noise variance at each observation. This is useful for stochastic simulators. Default is NULL.\", \n  .             estim.method = \"Argument of: DiceKriging::km\\n\\na character string specifying the method by which unknown parameters are estimated. Default is \\\"MLE\\\" (Maximum Likelihood). At this stage, a beta version of leave-One-Out estimation (estim.method=\\\"LOO\\\") is also implemented for noise-free observations.\", \n  .             optim.method = \"Argument of: DiceKriging::km\\n\\nan optional character string indicating which optimization method is chosen for the likelihood maximization. \\\"BFGS\\\" is the optim quasi-Newton procedure of package stats, with the method \\\"L-BFGS-B\\\". \\\"gen\\\" is the genoud genetic algorithm (using derivatives) from package rgenoud (>= 5.3.3).\", \n  .             lower = \"Argument of: DiceKriging::km\\n\\n(see below)\", \n  .             upper = \"Argument of: DiceKriging::km\\n\\noptional vectors containing the bounds of the correlation parameters for optimization. The default values are given by covParametersBounds.\", \n  .             parinit = \"Argument of: DiceKriging::km\\n\\nan optional vector containing the initial values for the variables to be optimized over. If no vector is given, an initial point is generated as follows. For method \\\"gen\\\", the initial point is generated uniformly inside the hyper-rectangle domain defined by lower and upper. For method \\\"BFGS\\\", some points (see control below) are generated uniformly in the domain. Then the best point with respect to the likelihood (or penalized likelihood, see penalty) criterion is chosen.\", \n  .             multistart = \"Argument of: DiceKriging::km\\n\\nan optional integer indicating the number of initial points from which running the BFGS optimizer. These points will be selected as the best multistart one(s) among those evaluated (see above parinit). The multiple optimizations will be performed in parallel provided that a parallel backend is registered (see package foreach).\", \n  .             control = \"Argument of: DiceKriging::km\\n\\nan optional list of control parameters for optimization. See details below.\", \n  .             gr = \"Argument of: DiceKriging::km\\n\\nan optional boolean indicating whether the analytical gradient should be used. Default is TRUE.\", \n  .             iso = \"Argument of: DiceKriging::km\\n\\nan optional boolean that can be used to force a tensor-product covariance structure (see covTensorProduct-class) to have a range parameter common to all dimensions. Default is FALSE. Not used (at this stage) for the power-exponential type.\", \n  .             scaling = \"Argument of: DiceKriging::km\\n\\nan optional boolean indicating whether a scaling on the covariance structure should be used.\", \n  .             knots = \"Argument of: DiceKriging::km\\n\\nan optional list of knots for scaling. The j-th element is a vector containing the knots for dimension j. If scaling=TRUE and knots are not specified, than knots are fixed to 0 and 1 in each dimension (which corresponds to affine scaling for the domain [0,1]^d).\"), \n  .         config = list(on.learner.error = \"stop\"), fix.factors.prediction = TRUE), class = c(\"regr.km\", \n  .     \"RLearnerRegr\", \"RLearner\", \"Learner\")), learner.model = new(\"km\", \n  .         d = 4L, n = 16L, X = structure(c(-1, -1, -1, -1, -1, \n  .         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 7962, 3666, \n  .         4076, 5951, 4777, 7275, 6141, 6426, 7505, 6001, 1143, \n  .         3673, 5193, 6312, 5972, 5887, 1189, 1360, 1704, 289, \n  .         1950, 162, 2750, 2059, 2944, 987, 457, 1578, 33, 1416, \n  .         1130, 554, 9, 15, 4, 16, 6, 6, 11, 19, 8, 14, 15, 5, \n  .         4, 7, 18, 18), dim = c(16L, 4L), dimnames = list(NULL, \n  .             c(\"cp\", \"minsplit\", \"minbucket\", \"maxdepth\"))), y = structure(c(0, \n  .         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), dim = c(16L, \n  .         1L)), p = 1L, F = structure(c(1, 1, 1, 1, 1, 1, 1, 1, \n  .         1, 1, 1, 1, 1, 1, 1, 1), dim = c(16L, 1L), dimnames = list(\n  .             c(\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \n  .             \"11\", \"12\", \"13\", \"14\", \"15\", \"16\"), \"(Intercept)\"), assign = 0L), \n  .         trend.formula = ~1, trend.coef = 0, covariance = new(\"covTensorProduct\", \n  .             d = 4L, name = \"matern3_2\", paramset.n = 1L, var.names = c(\"cp\", \n  .             \"minsplit\", \"minbucket\", \"maxdepth\"), sd2 = 0, known.covparam = \"None\", \n  .             nugget.flag = FALSE, nugget.estim = FALSE, nugget = numeric(0), \n  .             param.n = 4L, range.n = 4L, range.names = \"theta\", \n  .             range.val = c(4.71077734138817e-11, 3373.63962680901, \n  .             4156.72535271759, 18.9674736606325), shape.n = integer(0), \n  .             shape.names = character(0), shape.val = numeric(0)), \n  .         noise.flag = FALSE, noise.var = numeric(0), known.param = \"None\", \n  .         case = \"LLconcentration_beta_sigma2\", param.estim = TRUE, \n  .         method = \"MLE\", penalty = list(), optim.method = \"BFGS\", \n  .         lower = c(1e-10, 1e-10, 1e-10, 1e-10), upper = c(0, 13638, \n  .         5822, 30), control = list(trace = 3, multistart = 1, \n  .             pop.size = 20, upper.alpha = 0.99999999, convergence = 52L), \n  .         gr = TRUE, call = (function (formula = ~1, design, response, \n  .             covtype = \"matern5_2\", coef.trend = NULL, coef.cov = NULL, \n  .             coef.var = NULL, nugget = NULL, nugget.estim = FALSE, \n  .             noise.var = NULL, estim.method = \"MLE\", penalty = NULL, \n  .             optim.method = \"BFGS\", lower = NULL, upper = NULL, \n  .             parinit = NULL, multistart = 1, control = NULL, gr = TRUE, \n  .             iso = FALSE, scaling = FALSE, knots = NULL, kernel = NULL) \n  .         {\n  .             if (!is.null(kernel)) {\n  .                 covtype <- \"covUser\"\n  .                 nugget.estim <- FALSE\n  .             }\n  .             model <- new(\"km\")\n  .             model@call <- match.call()\n  .             data <- data.frame(design)\n  .             model@trend.formula <- formula <- drop.response(formula, \n  .                 data = data)\n  .             F <- model.matrix(formula, data = data)\n  .             X <- as.matrix(data)\n  .             y <- as.matrix(response)\n  .             model@X <- X\n  .             model@y <- y\n  .             model@d <- ncol(X)\n  .             model@n <- nrow(X)\n  .             model@F <- F\n  .             model@p <- ncol(F)\n  .             model@noise.flag <- (length(noise.var) != 0)\n  .             model@noise.var <- as.numeric(noise.var)\n  .             isTrend <- length(coef.trend) != 0\n  .             isCov <- length(coef.cov) != 0\n  .             isVar <- length(coef.var) != 0\n  .             if ((scaling) & (length(knots) > 0)) {\n  .                 knots <- checkNamesList(X1 = X, l2 = knots)\n  .             }\n  .             if ((isTrend && isCov && isVar) || (covtype == \"covUser\")) {\n  .                 known.param <- \"All\"\n  .                 nugget.estim <- FALSE\n  .             }\n  .             else if ((isTrend) && ((!isCov) || (!isVar))) {\n  .                 known.param <- \"Trend\"\n  .             }\n  .             else if ((!isTrend) && isCov && isVar) {\n  .                 known.param <- \"CovAndVar\"\n  .                 nugget.estim <- FALSE\n  .             }\n  .             else {\n  .                 known.param <- \"None\"\n  .                 coef.var <- coef.cov <- NULL\n  .             }\n  .             if (isCov) {\n  .                 known.covparam <- \"All\"\n  .             }\n  .             else {\n  .                 known.covparam <- \"None\"\n  .             }\n  .             if (isTRUE(scaling) && is.null(knots)) \n  .                 knots <- as.list(data.frame(rbind(apply(FUN = min, \n  .                   2, X = design), apply(FUN = max, 2, X = design))))\n  .             model@covariance <- covStruct.create(covtype = covtype, \n  .                 d = model@d, known.covparam = known.covparam, \n  .                 var.names = colnames(X), coef.cov = coef.cov, \n  .                 coef.var = coef.var, nugget = nugget, nugget.estim = nugget.estim, \n  .                 nugget.flag = ((length(nugget) != 0) || nugget.estim), \n  .                 iso = iso, scaling = scaling, knots = knots, \n  .                 kernel = kernel)\n  .             model@known.param <- known.param\n  .             if (known.param == \"All\") {\n  .                 model@trend.coef <- as.numeric(coef.trend)\n  .                 model@param.estim <- FALSE\n  .                 validObject(model)\n  .                 model <- computeAuxVariables(model)\n  .                 return(model)\n  .             }\n  .             if (known.param == \"CovAndVar\") {\n  .                 model@param.estim <- TRUE\n  .                 validObject(model)\n  .                 model <- computeAuxVariables(model)\n  .                 x <- backsolve(t(model@T), model@y, upper.tri = FALSE)\n  .                 beta <- compute.beta.hat(x = x, M = model@M)\n  .                 z <- compute.z(x = x, M = model@M, beta = beta)\n  .                 model@z <- z\n  .                 model@trend.coef <- beta\n  .                 return(model)\n  .             }\n  .             if (known.param == \"Trend\") {\n  .                 model@trend.coef <- as.numeric(coef.trend)\n  .             }\n  .             if (length(penalty) == 0) {\n  .                 if (is.element(estim.method, c(\"MLE\", \"LOO\"))) {\n  .                   model@method <- estim.method\n  .                 }\n  .                 else {\n  .                   stop(\"estim.method must be: 'MLE' or 'LOO'\")\n  .                 }\n  .             }\n  .             else {\n  .                 if (covtype != \"gauss\") {\n  .                   stop(\"At this stage, Penalized Maximum Likelihood is coded only for Gaussian covariance\")\n  .                 }\n  .                 penalty.set <- c(\"SCAD\")\n  .                 if (!is.element(penalty$fun, penalty.set)) {\n  .                   stop(\"At this stage, the penalty #function has to be one of : SCAD\")\n  .                 }\n  .                 if (length(penalty$value) == 0) {\n  .                   penalty$value <- sqrt(2 * log(model@n)/model@n) * \n  .                     seq(from = 1, by = 0.5, length = 15)\n  .                 }\n  .                 penalty$fun.derivative <- paste(penalty$fun, \n  .                   \".derivative\", sep = \"\")\n  .                 model@penalty <- penalty\n  .                 model@method <- \"PMLE\"\n  .             }\n  .             model@param.estim <- TRUE\n  .             model@optim.method <- as.character(optim.method)\n  .             if ((length(lower) == 0) || (length(upper) == 0)) {\n  .                 bounds <- covParametersBounds(model@covariance, \n  .                   design)\n  .                 if (length(lower) == 0) \n  .                   lower <- bounds$lower\n  .                 if (length(upper) == 0) \n  .                   upper <- bounds$upper\n  .             }\n  .             if ((multistart > 1) && (optim.method == \"gen\")) {\n  .                 warning(\"The 'multistart' argument is not used when 'optim.method' is 'gen'.\")\n  .                 multistart <- 1\n  .             }\n  .             control$multistart <- multistart\n  .             model@lower <- as.numeric(lower)\n  .             model@upper <- as.numeric(upper)\n  .             model@parinit <- as.numeric(parinit)\n  .             if (optim.method == \"BFGS\") {\n  .                 if (length(control$pop.size) == 0) \n  .                   control$pop.size <- 20\n  .                 control$pop.size <- max(control$pop.size, multistart)\n  .                 if (identical(control$trace, FALSE)) \n  .                   control$trace <- 0\n  .                 if ((length(control$trace) == 0) || (identical(control$trace, \n  .                   TRUE))) {\n  .                   control$trace <- 3\n  .                 }\n  .             }\n  .             if (optim.method == \"gen\") {\n  .                 d <- ncol(design)\n  .                 if (length(control$pop.size) == 0) \n  .                   control$pop.size <- min(20, floor(4 + 3 * log(d)))\n  .                 if (length(control$max.generations) == 0) \n  .                   control$max.generations <- 5\n  .                 if (length(control$wait.generations) == 0) \n  .                   control$wait.generations <- 2\n  .                 if (length(control$BFGSburnin) == 0) \n  .                   control$BFGSburnin <- 0\n  .                 if (identical(control$trace, FALSE)) {\n  .                   control$trace <- 0\n  .                 }\n  .                 else control$trace <- 1\n  .             }\n  .             upper.alpha <- control$upper.alpha\n  .             if (length(upper.alpha) == 0) {\n  .                 control$upper.alpha <- 1 - 1e-08\n  .             }\n  .             else if ((upper.alpha < 0) || (upper.alpha > 1)) {\n  .                 control$upper.alpha <- 1 - 1e-08\n  .             }\n  .             model@control <- control\n  .             model@gr <- as.logical(gr)\n  .             envir.logLik <- new.env()\n  .             validObject(model, complete = TRUE)\n  .             varStationaryClass <- c(\"covTensorProduct\", \"covScaling\", \n  .                 \"covIso\")\n  .             if (length(noise.var) != 0) {\n  .                 model@case <- \"LLconcentration_beta\"\n  .             }\n  .             else if (!is.element(class(model@covariance), varStationaryClass)) {\n  .                 model@case <- \"LLconcentration_beta\"\n  .             }\n  .             else {\n  .                 knownNugget <- ((length(nugget) > 0) && (!nugget.estim))\n  .                 if (nugget.estim) {\n  .                   model@case <- \"LLconcentration_beta_v_alpha\"\n  .                 }\n  .                 else if (knownNugget) {\n  .                   model@case <- \"LLconcentration_beta\"\n  .                 }\n  .                 else {\n  .                   model@case <- \"LLconcentration_beta_sigma2\"\n  .                 }\n  .             }\n  .             if ((model@method == \"LOO\") & (model@case != \"LLconcentration_beta_sigma2\")) {\n  .                 stop(\"leave-One-Out is not available for this model\")\n  .             }\n  .             f <- kmEstimate\n  .             if (identical(model@method, \"PMLE\")) {\n  .                 cv <- function(lambda, object, f) {\n  .                   object@penalty$value <- lambda\n  .                   object@control$trace <- 0\n  .                   object <- f(object, envir = envir.logLik)\n  .                   criterion <- sum((object@y - leaveOneOut.km(object, \n  .                     type = \"UK\")$mean)^2)\n  .                   return(criterion)\n  .                 }\n  .                 lambda.val <- model@penalty$value\n  .                 nval <- length(lambda.val)\n  .                 u <- rep(0, nval)\n  .                 for (i in 1L:nval) {\n  .                   u[i] <- cv(lambda.val[i], object = model, f)\n  .                 }\n  .                 lambda <- lambda.val[which.min(u)]\n  .                 model@penalty$value <- lambda\n  .                 model <- f(model, envir = envir.logLik)\n  .             }\n  .             else {\n  .                 model <- f(model, envir = envir.logLik)\n  .             }\n  .             return(model)\n  .         })(design = structure(list(cp = c(-1, -1, -1, -1, -1, \n  .         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1), minsplit = c(7962, \n  .         3666, 4076, 5951, 4777, 7275, 6141, 6426, 7505, 6001, \n  .         1143, 3673, 5193, 6312, 5972, 5887), minbucket = c(1189, \n  .         1360, 1704, 289, 1950, 162, 2750, 2059, 2944, 987, 457, \n  .         1578, 33, 1416, 1130, 554), maxdepth = c(9, 15, 4, 16, \n  .         6, 6, 11, 19, 8, 14, 15, 5, 4, 7, 18, 18)), class = \"data.frame\", row.names = c(NA, \n  .         -16L)), response = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n  .         0, 0, 0, 0, 0), covtype = \"matern3_2\", control = list(\n  .             trace = TRUE)), parinit = c(4.71077734138817e-11, \n  .         3373.63962680901, 4156.72535271759, 18.9674736606325), \n  .         logLik = 0, T = structure(c(0, 0, 0, 0, 0, 0, 0, 0, 0, \n  .         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n  .         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n  .         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n  .         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n  .         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n  .         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n  .         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n  .         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n  .         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n  .         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n  .         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n  .         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n  .         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n  .         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), dim = c(16L, \n  .         16L)), z = c(NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, \n  .         NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN), M = structure(c(Inf, \n  .         Inf, Inf, Inf, -Inf, Inf, Inf, Inf, Inf, -Inf, Inf, -Inf, \n  .         Inf, -Inf, Inf, Inf), dim = c(16L, 1L))), task.desc = structure(list(\n  .         id = \"data\", type = \"regr\", target = \"y\", size = 16L, \n  .         n.feat = c(numerics = 4L, factors = 0L, ordered = 0L, \n  .         functionals = 0L), has.missings = FALSE, has.weights = FALSE, \n  .         has.blocking = FALSE, has.coordinates = FALSE), class = c(\"RegrTaskDesc\", \n  .     \"SupervisedTaskDesc\", \"TaskDesc\")), subset = 1:16, features = c(\"cp\", \n  .     \"minsplit\", \"minbucket\", \"maxdepth\"), factor.levels = structure(list(), names = character(0)), \n  .     time = 0.0150000000003274, dump = NULL), class = \"WrappedModel\"), \n  .     .newdata = structure(list(cp = c(-1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, \n  .     -1, -1, -1, -1), minsplit = c(2697, 5731, 3956, 6665, 7722, \n  .     6654, 6679, 4379, 6587, 4188, 4088, 4942, 3139, 6621, 5482, \n  .     6079, 2172, 4347, 5092, 5143, 4289, 7393, 6094, 4563, 5167, \n  .     5772, 3618, 6884, 7595, 3902, 6449, 3293, 5808, 6980, 4767, \n  .     6118, 6128, 6109, 6304, 5066, 780, 6250, 3661, 6542, 1468, \n  .     7455, 5452, 3445, 4542, 5332, 6645, 2501, 7544, 7046, 3501, \n  .     7869, 5008, 6659, 8000, 3824, 4666, 7820, 7555, 4653, 4385, \n  .     740, 4050, 4952, 5022, 6771, 6369, 7384, 2931, 1969, 6504, \n  .     7989, 4062, 3187, 4036, 4420, 6968, 4230, 3248, 7209, 1113, \n  .     7159, 3863, 5833, 7966, 6387, 3728, 6858, 477, 5460, 4096, \n  .     1841, 5564, 6560, 2882, 6742, 7795, 6901, 3275, 7617, 3154, \n  .     7936, 7980, 5843, 6564, 1695, 7198, 7832, 5112, 1854, 2100, \n  .     6733, 6906, 7625, 3225, 3526, 3179, 4341, 7835, 4868, 1337, \n  .     4635, 4330, 4799, 4723, 6535, 7357, 7734, 5724, 7636, 4978, \n  .     7690, 1997, 6628, 767, 5693, 5931, 1860, 6595, 2006, 5856, \n  .     5758, 5465, 4687, 3072, 3691, 4256, 5603, 5528, 6054, 7271, \n  .     5389, 7858, 3906, 1320, 6694, 1785, 656, 4884, 7811, 7415, \n  .     3392, 7016, 5373, 7651, 4097, 5915, 7519, 5582, 4106, 5938, \n  .     4065, 4792, 7438, 3014, 6924, 4014, 7007, 7446, 2806, 5010, \n  .     6745, 5150, 2894, 7806, 7670, 4902, 4926, 4077, 4629, 4221, \n  .     6976, 5879, 6158, 3496, 5642, 5054, 6528, 6057, 7420, 3815, \n  .     4142, 5886, 2108, 6378, 7365, 5309, 5965, 2487, 7687, 3871, \n  .     7283, 7176, 5380, 6145, 3982, 1924, 1877, 7579, 6001, 7070, \n  .     7144, 7854, 7784, 5550, 5744, 5559, 5589, 7346, 668, 6218, \n  .     6687, 6170, 5782, 1936, 6032, 7117, 2344, 7290, 4604, 1909, \n  .     5430, 6405, 3434, 5890, 7604, 7236, 4495, 2860, 6840, 3654, \n  .     5226, 2651, 7766, 5517, 4919, 6458, 4126, 6216, 4929, 5627, \n  .     4784, 4147, 2941, 7772, 7031, 5600, 7126, 6433, 3762, 2644, \n  .     6708, 7616, 2153, 7664, 4731, 6603, 7273, 2563, 5956, 4409, \n  .     7751, 3739, 6805, 4284, 6790, 4954, 7946, 7177, 1402, 7943, \n  .     7522, 3920, 5948, 2720, 2091, 3802, 5789, 2768, 5265, 4546, \n  .     5117, 696, 7136, 7219, 6468, 3894, 2591, 4468, 6639, 6309, \n  .     6316, 3429, 7325, 7105, 6207, 7391, 1498, 3302, 6830, 2253, \n  .     7246, 5815, 7576, 7501, 6866, 7192, 6264, 5421, 3058, 6100, \n  .     6756, 4310, 4617, 7697, 5489, 3516, 3262, 7896, 1563, 5974, \n  .     2910, 2311, 7885, 4911, 4030, 2684, 2880, 7428, 5674, 5537, \n  .     2419, 7344, 7505, 4810, 2612, 6893, 6820, 4261, 7332, 7846, \n  .     4963, 6761, 5649, 6023, 3164, 3552, 3456, 6337, 6358, 6987, \n  .     3474, 7163, 7708, 1946, 4373, 4276, 6283, 6874, 5746, 6138, \n  .     7314, 5446, 7897, 5985, 5169, 6083, 6478, 7304, 4558, 6181, \n  .     1323, 4856, 7791, 3970, 7954, 5042, 7471, 4315, 7919, 6325, \n  .     2142, 6010, 3134, 7306, 6950, 5186, 1398, 4989, 2358, 7057, \n  .     1701, 3687, 6816, 4847, 7907, 2207, 7150, 3631, 6916, 3825, \n  .     2984, 5665, 3382, 6722, 951, 7205, 5135, 1445, 6929, 4773, \n  .     7545, 2046, 7093, 7102, 7232, 5658, 7590, 6997, 6955, 2333, \n  .     6425, 7017, 2058, 4197, 7459, 6612, 4678, 6943, 6508, 1609, \n  .     3645, 3481, 6490, 4462, 6424, 7879, 2429, 6248, 7529, 2964, \n  .     3470, 6781, 7678, 6065, 3997, 4180, 7715, 5327, 7050, 2829, \n  .     5998, 4209, 5909, 2165, 3962, 3557, 6270, 4205, 7252, 7562, \n  .     7642, 2837, 1254, 3024, 3670, 6551, 3401, 2275, 7481, 7478, \n  .     6845, 3533, 7925, 5709, 7406, 589, 7370, 6131, 7969, 7743, \n  .     6415, 6571, 5270, 3353, 4469, 7151, 7676, 6932, 6806, 3501, \n  .     7556, 5011, 1357, 2034, 3865, 6690, 4841, 5367, 7839, 2072, \n  .     6563, 5427, 4101, 1867, 3709, 5347, 7418, 6852, 4624, 2501, \n  .     5630, 4187, 4709, 4345, 7065, 2601, 1628, 7885, 4214, 1712, \n  .     5752, 4513, 5767, 6724, 646, 7309, 7531, 4487, 4284, 5609, \n  .     6093, 5582, 5968, 7905, 6218, 7740, 6964, 3791, 4867, 6197, \n  .     5851, 2539, 5134, 5591, 5634, 4927, 2912, 5325, 6001, 3107, \n  .     6356, 6792, 4816, 2485, 2779, 1685, 4532, 4300, 1471, 7804, \n  .     4682, 1856, 3763, 6498, 5524, 4399, 5128, 5093, 5796, 6887, \n  .     6777, 3777, 6283, 3827, 6315, 4504, 4763, 3476, 7121, 7821, \n  .     7016, 6460, 7039, 5419, 6428, 6575, 7087, 5042, 7162, 7214, \n  .     6748, 7630, 4023, 1766, 7009, 2586, 5980, 5703, 5876, 7183, \n  .     2300, 7053, 6420, 7824, 2341, 1378, 7408, 6121, 7985, 7483, \n  .     3658, 7964, 7093, 7654, 3724, 5309, 4598, 6180, 6150, 6022, \n  .     3810, 7897, 4971, 7684, 3845, 2751, 3642, 5657, 4871, 7479, \n  .     7336, 5733, 5862, 3538, 7972, 6983, 7292, 7334, 2115, 5111, \n  .     1601, 3225, 7715, 5491, 6718, 6845, 4227, 5916, 6306, 4258, \n  .     4660, 6877, 6609, 7399, 6520, 4327, 7512, 6645, 1671, 6107, \n  .     6631, 6158, 7769, 7358, 2666, 7775, 7924, 3931, 7273, 1100, \n  .     7705, 2178, 4962, 5815, 6383, 5479, 1814, 7525, 6006, 6340, \n  .     7741, 5788, 1488, 3170, 3317, 5211, 7855, 746, 7449, 6954, \n  .     7437, 5543, 5151, 2957, 4179, 3408, 7224, 7596, 4451, 1941, \n  .     7265, 2371, 7138, 7581, 7195, 3374, 6481, 7621, 2739, 2840, \n  .     6458, 6912, 6662, 6213, 7574, 2319, 6602, 6078, 6043, 4390, \n  .     7950, 3977, 5242, 6676, 4897, 6364, 6063, 4247, 2355, 3737, \n  .     3957, 6989, 5619, 6577, 1428, 7043, 7801, 6386, 5732, 5154, \n  .     2758, 7958, 3761, 5083, 6715, 6888, 7641, 7979, 7123, 4411, \n  .     3013, 4834, 4248, 7364, 6248, 2576, 5659, 4926, 4615, 3530, \n  .     7693, 5334, 6615, 1734, 3638, 7323, 5975, 7875, 856, 6070, \n  .     5432, 7423, 6484, 3141, 5765, 5100, 2686, 6850, 5951, 7810, \n  .     7479, 5567, 1989, 6018, 7208, 4045, 2728, 6811, 4109, 6953, \n  .     7519, 5298, 7007, 3083, 4117, 7316, 6353, 4498, 5459, 5262, \n  .     5390, 7133, 4375, 5123, 4322, 5373, 5787, 7176, 7258, 5507, \n  .     7757, 2951, 6440, 3155, 6791, 5917, 6704, 3211, 5050, 4167, \n  .     5204, 1603, 7078, 6211, 3833, 7555, 7864, 5696, 1303, 5247, \n  .     7584, 3373, 6930, 6467, 5007, 3268, 4013, 7490, 6553, 5814, \n  .     2997, 7724, 4979, 7250, 6314, 7407, 7916, 5872, 6646, 5687, \n  .     3056, 7663, 4154, 7043, 5908, 5150, 7564, 2018, 7963, 5478, \n  .     5070, 6337, 2491, 5309, 4434, 7711, 6508, 5961, 7268, 3139, \n  .     5743, 3816, 4542, 7241, 788, 7097, 5606, 7422, 3268, 2851, \n  .     6222, 5217, 2746, 6078, 6928, 6931, 7845, 3286, 7179, 7352, \n  .     6763, 5383, 6810, 4858, 3696, 4067, 6006, 7766, 6703, 367, \n  .     3373, 3919, 5257, 3635, 4657, 7452, 7877, 4996, 2833, 7524, \n  .     6394, 7409, 7553, 4685, 6167, 2906, 5519, 6438, 5445, 7057, \n  .     3013, 4013, 7955, 7764, 5362, 4444, 6878, 6280, 5724, 5101, \n  .     7667, 3651, 7370, 6522, 2225, 6840, 1826, 4623, 4830, 5201, \n  .     1178, 4414, 7031, 2812, 7094, 7756, 2929, 7905, 5148, 6439, \n  .     1763, 7400, 1215, 5944, 6779, 3736, 2883, 7078, 5114, 5869, \n  .     3953, 5009, 7773, 6684, 6536, 7733, 5135, 7205, 4059, 7372, \n  .     5190, 5699, 1463, 6841, 4935), minbucket = c(1026, 866, 683, \n  .     2571, 977, 732, 1486, 317, 3284, 1403, 1540, 1941, 668, 2046, \n  .     1282, 2470, 472, 723, 2488, 1480, 396, 1700, 1365, 189, 670, \n  .     2116, 1429, 1550, 2415, 775, 2859, 1397, 2517, 1560, 994, \n  .     789, 1662, 976, 450, 315, 157, 570, 365, 1460, 212, 3565, \n  .     2224, 220, 331, 1946, 1493, 992, 3247, 87, 1227, 2005, 1032, \n  .     1508, 3168, 199, 857, 3059, 1251, 1930, 438, 74, 1933, 1424, \n  .     341, 2105, 1309, 2397, 591, 377, 2637, 725, 287, 99, 944, \n  .     1182, 205, 1504, 361, 2122, 234, 1790, 850, 2281, 1119, 2143, \n  .     1180, 2536, 141, 113, 473, 299, 2760, 279, 763, 3139, 3054, \n  .     3438, 1141, 41, 203, 321, 2064, 1639, 1498, 664, 2953, 2053, \n  .     748, 561, 81, 2291, 1694, 1906, 134, 1306, 752, 1335, 3195, \n  .     1793, 506, 1350, 558, 778, 1732, 2420, 3180, 1213, 147, 2095, \n  .     225, 2887, 435, 1369, 275, 2497, 2607, 101, 2216, 771, 2540, \n  .     810, 1573, 2193, 633, 512, 122, 504, 1055, 463, 679, 1705, \n  .     3907, 1520, 61, 1052, 128, 262, 2313, 2820, 542, 458, 1831, \n  .     2160, 1298, 1920, 1133, 3127, 605, 1418, 1042, 129, 430, \n  .     1545, 1356, 760, 52, 2133, 2649, 112, 1815, 2318, 358, 604, \n  .     2587, 3016, 2266, 614, 997, 551, 1671, 581, 1040, 1268, 595, \n  .     325, 1292, 822, 2560, 3156, 425, 1882, 712, 903, 1123, 1973, \n  .     2070, 2774, 247, 2482, 1629, 105, 2042, 374, 340, 1145, 481, \n  .     889, 2252, 2647, 3489, 3067, 1319, 3530, 2746, 1456, 1413, \n  .     14, 3045, 54, 932, 1608, 2151, 1100, 548, 2451, 3018, 1126, \n  .     307, 691, 119, 1261, 2185, 568, 2771, 1897, 1220, 1799, 1412, \n  .     2051, 1587, 958, 805, 625, 2599, 168, 1428, 1582, 1744, 170, \n  .     744, 478, 289, 934, 788, 3474, 1985, 2734, 537, 48, 1316, \n  .     137, 2016, 599, 2850, 524, 2695, 2463, 576, 1623, 1716, 95, \n  .     1208, 2139, 1087, 2035, 553, 2939, 2259, 490, 3225, 1001, \n  .     579, 1244, 20, 40, 954, 2812, 309, 643, 2117, 486, 214, 926, \n  .     3005, 3197, 828, 1176, 1188, 1724, 534, 2562, 1380, 1725, \n  .     1338, 687, 1717, 421, 1111, 2941, 693, 1108, 10, 797, 1591, \n  .     707, 1286, 2621, 1536, 638, 920, 3079, 830, 230, 2862, 2101, \n  .     193, 803, 2219, 304, 1865, 372, 34, 2781, 1160, 408, 870, \n  .     1406, 185, 2371, 1224, 1204, 89, 2402, 610, 848, 2405, 1635, \n  .     1302, 2303, 3267, 1844, 1481, 1364, 2990, 1007, 1194, 346, \n  .     1140, 2611, 3487, 30, 588, 652, 841, 1596, 2100, 1247, 1280, \n  .     70, 2728, 734, 2321, 2191, 2409, 961, 1774, 1846, 1618, 26, \n  .     2110, 6, 2296, 1023, 1209, 2743, 906, 2204, 1471, 1965, 1171, \n  .     1066, 2145, 414, 1770, 1735, 887, 153, 222, 149, 1981, 767, \n  .     494, 2910, 1911, 2788, 987, 2949, 243, 3109, 1232, 21, 1089, \n  .     1260, 1645, 400, 1686, 381, 283, 1570, 1666, 910, 899, 420, \n  .     2239, 2447, 2280, 3114, 879, 1627, 818, 1325, 1167, 719, \n  .     1434, 2330, 351, 876, 183, 237, 249, 862, 1276, 3170, 1853, \n  .     2129, 2739, 499, 716, 2177, 1473, 1296, 2673, 1711, 894, \n  .     442, 1153, 1529, 1387, 2479, 1073, 2732, 1101, 447, 967, \n  .     648, 178, 2881, 174, 1162, 2, 515, 293, 60, 268, 161, 3024, \n  .     1270, 258, 1817, 1359, 2584, 620, 3031, 883, 3289, 67, 1786, \n  .     1568, 453, 938, 1132, 79, 2517, 1325, 1155, 2635, 1541, 2322, \n  .     515, 2, 3747, 2393, 646, 304, 991, 1131, 448, 661, 2725, \n  .     312, 1164, 1120, 1745, 240, 716, 599, 3113, 854, 2115, 941, \n  .     617, 507, 753, 547, 830, 1217, 466, 1343, 1239, 117, 2496, \n  .     1893, 356, 371, 187, 1078, 1561, 229, 1529, 2619, 2748, 434, \n  .     1291, 1360, 572, 2589, 731, 628, 821, 2946, 1453, 15, 1774, \n  .     2058, 523, 692, 1140, 1863, 414, 902, 3124, 87, 164, 1196, \n  .     878, 285, 177, 1914, 220, 1173, 1193, 256, 985, 2565, 1608, \n  .     1713, 2412, 263, 2878, 473, 1305, 374, 772, 1042, 2795, 1735, \n  .     1764, 1667, 2760, 650, 2296, 844, 2824, 1526, 1070, 2364, \n  .     1857, 1488, 157, 1373, 3349, 2185, 1402, 806, 216, 1023, \n  .     1622, 972, 319, 1013, 1051, 245, 798, 3735, 128, 142, 342, \n  .     1249, 2771, 1309, 1653, 2430, 924, 1786, 73, 2474, 1838, \n  .     1965, 1587, 1760, 1512, 1110, 2313, 690, 113, 75, 1095, 1316, \n  .     1549, 3512, 3308, 2544, 2907, 1390, 1957, 740, 1423, 949, \n  .     423, 2339, 147, 641, 1500, 2229, 1056, 3336, 1262, 870, 2944, \n  .     189, 1024, 1976, 33, 1005, 2786, 1097, 3502, 3169, 395, 298, \n  .     1781, 203, 763, 60, 42, 206, 1659, 1899, 280, 99, 2464, 333, \n  .     891, 2692, 2085, 1281, 675, 384, 131, 502, 2006, 958, 17, \n  .     1382, 1571, 704, 621, 34, 602, 2956, 2809, 328, 862, 1207, \n  .     916, 1270, 2263, 3482, 2129, 50, 3081, 461, 2529, 817, 91, \n  .     784, 440, 2137, 481, 490, 2091, 1233, 399, 568, 2442, 276, \n  .     1926, 1409, 1578, 1674, 3631, 707, 1481, 1254, 1448, 418, \n  .     2046, 977, 700, 974, 377, 2225, 803, 523, 628, 80, 2664, \n  .     2633, 1964, 1537, 367, 2302, 1384, 939, 11, 325, 452, 3063, \n  .     1696, 1432, 1502, 1859, 460, 317, 1928, 284, 2282, 642, 32, \n  .     1633, 1372, 2018, 697, 470, 231, 1712, 1950, 3249, 423, 2704, \n  .     780, 1193, 2111, 582, 1334, 1052, 184, 1561, 1066, 1893, \n  .     1746, 1135, 835, 146, 198, 103, 275, 164, 1263, 2957, 84, \n  .     2513, 944, 1393, 1292, 1245, 2293, 249, 158, 2214, 1870, \n  .     1926, 1103, 46, 655, 2478, 1093, 1554, 2359, 2419, 732, 1081, \n  .     1231, 1468, 972, 913, 2592, 412, 594, 226, 671, 356, 3306, \n  .     1812, 396, 887, 3711, 615, 562, 487, 2879, 1158, 1168, 63, \n  .     1212, 996, 517, 848, 2454, 1845, 713, 3563, 751, 2259, 3030, \n  .     536, 1978, 1309, 1362, 1913, 447, 3275, 1621, 1706, 212, \n  .     1773, 378, 419, 2583, 945, 403, 1734, 271, 858, 984, 2058, \n  .     1445, 170, 2320, 958, 2203, 7, 2017, 2119, 121, 1515, 80, \n  .     1871, 1566, 505, 1071, 1193, 1325, 808, 1983, 2644, 2977, \n  .     742, 1036, 1594, 550, 2385, 893, 312, 230, 1667, 1403, 2343, \n  .     2956, 44, 595, 1303, 1127, 347, 136, 2914, 3376, 1923, 841, \n  .     3470, 2996, 699, 2343, 423, 2868, 1273, 1734, 582, 1946, \n  .     3491, 315, 214, 3813, 868, 2426, 658, 140, 1654, 2288, 304, \n  .     969, 50, 2981, 1184, 552, 1222, 475, 1777, 1499, 2052, 122, \n  .     793, 1674, 121, 2098, 3803, 1330, 1501, 1579, 3069, 492, \n  .     256, 431, 61, 604, 879, 1040, 78, 788, 1717, 548, 213, 1214, \n  .     2633, 470, 2652, 145, 1021, 1591, 209, 2101, 2802, 420, 2008, \n  .     1236), maxdepth = c(16, 18, 20, 19, 10, 17, 13, 16, 3, 12, \n  .     9, 20, 6, 13, 3, 16, 12, 14, 4, 8, 14, 3, 9, 11, 3, 4, 9, \n  .     5, 16, 9, 12, 13, 11, 5, 14, 5, 12, 9, 15, 14, 17, 3, 8, \n  .     17, 11, 6, 20, 14, 6, 7, 7, 17, 8, 14, 15, 14, 12, 3, 14, \n  .     3, 11, 17, 14, 4, 16, 7, 12, 15, 13, 15, 5, 6, 19, 16, 5, \n  .     14, 12, 6, 16, 3, 14, 20, 5, 14, 19, 20, 14, 18, 5, 19, 3, \n  .     19, 8, 10, 7, 10, 18, 11, 8, 3, 7, 15, 9, 11, 16, 14, 12, \n  .     17, 13, 10, 7, 6, 6, 12, 6, 13, 3, 15, 7, 18, 16, 4, 15, \n  .     9, 10, 19, 3, 4, 12, 19, 19, 9, 3, 10, 9, 9, 6, 7, 15, 18, \n  .     10, 15, 7, 18, 8, 6, 16, 19, 19, 8, 20, 11, 4, 17, 17, 3, \n  .     16, 15, 4, 13, 20, 8, 20, 8, 8, 18, 8, 9, 16, 15, 20, 18, \n  .     10, 17, 14, 20, 10, 5, 6, 11, 20, 17, 19, 12, 8, 8, 3, 16, \n  .     17, 5, 7, 8, 18, 13, 7, 19, 15, 20, 15, 20, 4, 11, 13, 13, \n  .     18, 16, 7, 15, 19, 10, 12, 11, 11, 12, 11, 4, 3, 8, 12, 6, \n  .     13, 4, 8, 5, 3, 5, 12, 13, 4, 11, 6, 16, 18, 14, 6, 13, 17, \n  .     12, 11, 19, 12, 13, 18, 14, 7, 11, 17, 17, 9, 15, 15, 13, \n  .     8, 4, 7, 10, 20, 9, 12, 20, 5, 7, 5, 20, 12, 10, 13, 7, 20, \n  .     17, 13, 19, 11, 14, 3, 9, 19, 10, 18, 10, 5, 7, 20, 13, 6, \n  .     6, 9, 3, 17, 17, 18, 4, 14, 15, 8, 20, 13, 5, 13, 19, 9, \n  .     5, 11, 19, 11, 14, 7, 11, 7, 9, 10, 19, 12, 17, 5, 5, 16, \n  .     13, 4, 6, 9, 19, 11, 6, 14, 9, 8, 6, 17, 13, 10, 19, 12, \n  .     10, 13, 13, 7, 15, 18, 11, 18, 5, 9, 11, 4, 16, 14, 7, 10, \n  .     8, 3, 18, 6, 5, 14, 10, 12, 16, 14, 14, 4, 13, 10, 18, 14, \n  .     9, 18, 10, 19, 7, 5, 17, 3, 6, 9, 16, 3, 13, 15, 9, 6, 16, \n  .     13, 5, 8, 15, 20, 8, 10, 7, 6, 5, 20, 16, 13, 6, 3, 8, 15, \n  .     9, 20, 7, 10, 8, 15, 5, 15, 15, 18, 15, 11, 16, 20, 9, 10, \n  .     19, 18, 9, 15, 17, 18, 19, 10, 16, 11, 17, 19, 18, 12, 8, \n  .     17, 16, 18, 19, 20, 18, 13, 7, 16, 6, 10, 7, 18, 19, 4, 4, \n  .     12, 3, 4, 11, 19, 18, 9, 15, 20, 14, 14, 14, 14, 8, 9, 19, \n  .     9, 13, 9, 14, 3, 15, 10, 12, 3, 20, 3, 12, 20, 8, 15, 7, \n  .     19, 3, 16, 9, 11, 3, 4, 11, 3, 17, 17, 6, 5, 12, 14, 18, \n  .     19, 11, 5, 11, 19, 10, 9, 14, 12, 19, 7, 11, 7, 17, 20, 7, \n  .     5, 4, 11, 14, 16, 5, 16, 10, 13, 19, 4, 12, 5, 7, 4, 5, 14, \n  .     6, 15, 15, 10, 7, 4, 4, 20, 11, 17, 9, 15, 16, 20, 4, 5, \n  .     10, 13, 19, 10, 9, 10, 8, 5, 5, 13, 20, 9, 19, 13, 6, 14, \n  .     11, 16, 17, 3, 5, 12, 16, 3, 5, 18, 17, 14, 14, 12, 13, 17, \n  .     8, 15, 16, 7, 4, 18, 10, 7, 16, 3, 18, 7, 12, 19, 20, 6, \n  .     17, 15, 18, 12, 7, 17, 6, 15, 11, 3, 20, 11, 7, 14, 3, 15, \n  .     15, 20, 19, 6, 8, 20, 16, 8, 18, 4, 16, 4, 20, 17, 10, 16, \n  .     18, 4, 19, 7, 6, 3, 11, 3, 8, 5, 6, 15, 17, 10, 12, 3, 12, \n  .     5, 8, 6, 5, 9, 9, 18, 18, 14, 17, 10, 17, 9, 20, 11, 11, \n  .     15, 9, 4, 9, 8, 10, 12, 7, 4, 10, 19, 8, 3, 7, 11, 19, 4, \n  .     16, 12, 7, 19, 19, 7, 8, 5, 10, 17, 8, 19, 3, 13, 5, 20, \n  .     5, 20, 6, 9, 19, 11, 3, 20, 5, 8, 20, 19, 9, 9, 10, 10, 15, \n  .     13, 4, 10, 3, 14, 19, 14, 12, 15, 7, 8, 13, 11, 10, 8, 18, \n  .     20, 3, 9, 16, 20, 19, 4, 4, 3, 14, 20, 15, 11, 14, 14, 16, \n  .     6, 12, 6, 8, 19, 17, 4, 8, 8, 11, 16, 6, 17, 13, 17, 9, 18, \n  .     19, 4, 17, 13, 9, 6, 12, 15, 4, 17, 10, 7, 16, 9, 13, 8, \n  .     17, 6, 19, 5, 10, 15, 6, 15, 20, 5, 17, 18, 13, 9, 16, 15, \n  .     17, 18, 13, 11, 12, 14, 10, 13, 13, 8, 18, 6, 14, 9, 19, \n  .     18, 11, 11, 4, 6, 4, 3, 3, 5, 9, 15, 14, 12, 17, 18, 13, \n  .     15, 12, 14, 12, 19, 8, 19, 19, 4, 15, 20, 14, 14, 11, 19, \n  .     20, 15, 3, 5, 14, 6, 6, 12, 16, 8, 12, 20, 10, 15, 3, 7, \n  .     4, 3, 3, 14, 3, 5, 3, 10, 13, 6, 13, 8, 19, 5, 15, 15, 4, \n  .     5, 11, 19, 15, 13, 9, 5, 3, 18, 3, 5, 4, 20, 8, 3, 12, 16, \n  .     19, 3, 14, 18, 16, 17, 10, 15, 11, 5, 11, 20, 4, 16, 6, 16, \n  .     20, 13, 7, 15, 16, 6, 9, 15, 10, 20, 10, 7, 17, 12, 3, 13, \n  .     12, 17, 20, 18, 5, 17, 13, 13, 10, 16, 10, 7, 9, 4, 12, 6, \n  .     8, 7, 9, 15, 7, 11, 10, 16, 5, 12, 12, 17, 7, 16, 7, 17, \n  .     6, 4, 9, 20, 12, 8, 13, 9, 3, 9, 14, 14, 15, 18, 8, 15, 11, \n  .     20, 8, 4, 3, 12, 15, 16, 8, 5, 8, 19, 5, 3, 7, 13, 13, 10, \n  .     9, 18, 12, 14, 20, 13, 16, 4, 4, 10, 4, 16, 13, 10, 16, 14, \n  .     6, 10, 18)), row.names = c(NA, 1000L), class = \"data.frame\"), \n  .     jitter = FALSE)",
            "21. predictLearner(.learner, .model, .newdata, ...)",
            "22. predictLearner.regr.km(.learner, .model, .newdata, ...)",
            "23. DiceKriging::predict.km(.model$learner.model, newdata = .newdata, \n  .     type = \"SK\", se.compute = se)",
            "24. backsolve(t(T), c.newdata, upper.tri = FALSE)",
            "25. .handleSimpleError(function (cnd) \n  . {\n  .     watcher$capture_plot_and_output()\n  .     cnd <- sanitize_call(cnd)\n  .     watcher$push(cnd)\n  .     switch(on_error, continue = invokeRestart(\"eval_continue\"), \n  .         stop = invokeRestart(\"eval_stop\"), error = NULL)\n  . }, \"singular matrix in 'backsolve'. First zero in diagonal [1]\", \n  .     base::quote(backsolve(t(T), c.newdata, upper.tri = FALSE)))"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# almaceno los resultados de la Bayesian Optimization\n",
        "# y capturo los mejores hiperparametros encontrados\n",
        "\n",
        "tb_bayesiana <- as.data.table(bayesiana_salida$opt.path)\n",
        "\n",
        "# ordeno en forma descendente por AUC = y\n",
        "setorder(tb_bayesiana, -y)\n",
        "\n",
        "# grabo para eventualmente poder utilizarlos en OTRA corrida\n",
        "fwrite( tb_bayesiana,\n",
        "  file= \"BO_log.txt\",\n",
        "  sep= \"\\t\"\n",
        ")\n",
        "\n",
        "# los mejores hiperparámetros son los que quedaron en el registro 1 de la tabla\n",
        "PARAM$out$lgbm$mejores_hiperparametros <- tb_bayesiana[\n",
        "  1, # el primero es el de mejor AUC\n",
        "  list(cp, minsplit, minbucket, maxdepth)\n",
        "]\n",
        "\n",
        "print(PARAM$out$lgbm$mejores_hiperparametros)"
      ],
      "metadata": {
        "id": "8YvKC3Zf0kMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "format(Sys.time(), \"%a %b %d %X %Y\")"
      ],
      "metadata": {
        "id": "lgmvRHcfJpls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMr6Z1enOyd3"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ]
}